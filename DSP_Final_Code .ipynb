{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WvGeG5XP7m7x"
   },
   "source": [
    "# **IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J0AeWHSVvFzq"
   },
   "outputs": [],
   "source": [
    "# Importing all the neccessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, PowerTransformer, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from scipy.stats import randint, uniform\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTU_3MsC7yuv"
   },
   "source": [
    "# **ABOUT THE DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 602
    },
    "id": "PvKqslAuxWpQ",
    "outputId": "7b49055d-6782-4449-ed35-bd6a28087313"
   },
   "outputs": [],
   "source": [
    "# Loading dataset\n",
    "data = pd.read_csv(r\"Sample - Superstore.csv\", encoding='latin1')\n",
    "print(\"Data loaded successfully.\")\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kCGiXkobxmIk",
    "outputId": "d652a093-936e-448c-c5e2-61c521b81520"
   },
   "outputs": [],
   "source": [
    "# Exploring the dataset and its properties\n",
    "\n",
    "print(\"Size of the DataFrame (rows, columns):\", data.shape)\n",
    "print(\"\\nColumn Names:\", data.columns)\n",
    "\n",
    "#Checking for missing null values\n",
    "print(\"\\nMissing values:\")\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_v_kjW0c8FGY"
   },
   "source": [
    "# **DATA CLEANING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "46TiRm56xnAh",
    "outputId": "3764be6d-f511-4cfd-8f69-3f00625fcaae"
   },
   "outputs": [],
   "source": [
    "# Data Cleaning\n",
    "\n",
    "# Replace '/' with '-' in 'Order Date' and 'Ship Date' columns for string values\n",
    "for col in ['Order Date', 'Ship Date']:\n",
    "    if data[col].dtype == 'object':\n",
    "        data[col] = data[col].str.replace('/', '-', regex=False)\n",
    "    else:\n",
    "        print(f\"Column '{col}' is not of string type, skipping replacement.\")\n",
    "\n",
    "# Convert order date and ship date to datetime objects\n",
    "data['Order Date'] = pd.to_datetime(data['Order Date'], errors='coerce')\n",
    "data['Ship Date'] = pd.to_datetime(data['Ship Date'], errors='coerce')\n",
    "\n",
    "# Display the data types after conversion\n",
    "print(\"Data types after converting date columns:\")\n",
    "print(data[['Order Date', 'Ship Date']].dtypes)\n",
    "\n",
    "# Display the first few rows to see the effect of the conversion\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XwBD8zMU8PiD"
   },
   "source": [
    "# **EXPLORATORY DATA ANALYSIS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQZHtAxC811-"
   },
   "source": [
    "## Shipping Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "joNCxA0BzR5H",
    "outputId": "8674462f-f1e3-4f9f-d45f-76cbcce9b69b"
   },
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "df = data.copy()\n",
    "\n",
    "# Plot to show how many orders took how many shipping days\n",
    "df['Shipping Time'] = df['Ship Date'] - df['Order Date']\n",
    "df['Shipping Days'] = df['Shipping Time'].dt.days\n",
    "plt.figure(figsize=(8, 4))\n",
    "shipping_counts = df['Shipping Days'].value_counts().sort_index()\n",
    "\n",
    "plt.bar(shipping_counts.index, shipping_counts.values)\n",
    "plt.xlabel('Shipping Days')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_iv9A14S86PM"
   },
   "source": [
    "## Ship Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "Tb4bFg_b0FgU",
    "outputId": "e6366b38-9f89-4222-c7db-0d77c78a5772"
   },
   "outputs": [],
   "source": [
    "# Analysis for Ship Mode column\n",
    "\n",
    "print(\"Unique values for Ship mode: \\t \\n\", df['Ship Mode'].unique())\n",
    "\n",
    "#Pie Plot for Ship Mode\n",
    "segment_counts = df['Ship Mode'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.pie(segment_counts, labels=segment_counts.index, autopct='%1.2f%%',explode = [0.1, 0, 0,0], startangle=90, shadow=True)\n",
    "plt.title('Distribution of Ship Mode')\n",
    "plt.show()\n",
    "\n",
    "# Box Plot for Ship Mode\n",
    "plt.figure(figsize=(10, 5))\n",
    "modes = ['Same Day', 'First Class', 'Second Class','Standard Class']\n",
    "plt.boxplot([df[df['Ship Mode'] == mode]['Shipping Days'] for mode in modes], labels=modes)\n",
    "plt.xlabel('Ship Mode')\n",
    "plt.ylabel('Shipping days')\n",
    "plt.title(\"Box Plot for Ship Mode\")\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8fbyJQ89LQB"
   },
   "source": [
    "## Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "DDe5IuQA0F8K",
    "outputId": "963c574e-6a29-4446-ebb0-37f86a8d476b"
   },
   "outputs": [],
   "source": [
    "# Analysis of Segment column\n",
    "\n",
    "print(\"Unique values for column Segment : \\t \\n\", df['Segment'].unique())\n",
    "\n",
    "# Plot for the same\n",
    "segment_counts = df['Segment'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.pie(segment_counts, labels=segment_counts.index, autopct='%1.2f%%',explode = [0.1, 0, 0], startangle=90, shadow=True)\n",
    "plt.title('Distribution of Customer Segments')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Honu-TiX9Ofx"
   },
   "source": [
    "## Customer Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "1t7znLQD0GDp",
    "outputId": "dea43c6b-b2e6-43fa-bf12-35876ae106e7"
   },
   "outputs": [],
   "source": [
    "# Distribution of Number of Sales per Customer\n",
    "\n",
    "segment_counts = df['Customer Name'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.hist(segment_counts, bins=35, edgecolor='black')\n",
    "plt.title('Distribution of Number of Sales per Customer')\n",
    "plt.xlabel('Number of Sales')\n",
    "plt.ylabel('Frequency (Number of Customers)')\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "QWKMeqi60GJt",
    "outputId": "578c8635-b95e-4849-a76c-46d1668ecd1d"
   },
   "outputs": [],
   "source": [
    "# Number of Sales analysis\n",
    "\n",
    "# Top 10\n",
    "sale_count = df['Customer Name'].value_counts()\n",
    "sale_count_top10 = sale_count.head(10)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.bar(sale_count_top10.index, sale_count_top10.values)\n",
    "plt.title('Number of Sales per Customer (top 10)')\n",
    "plt.xlabel('Customer Name')\n",
    "plt.ylabel('Number of Sales')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Bottom 10\n",
    "sale_count_bot10 = sale_count.tail(10)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(sale_count_bot10.index, sale_count_bot10.values)\n",
    "plt.title('Number of Sales per Customer (bottom 10)')\n",
    "plt.xlabel('Customer Name')\n",
    "plt.ylabel('Number of Sales')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BTKdJ2y9Tij"
   },
   "source": [
    "## State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "93YHN9qM0GPi",
    "outputId": "25d0f0fc-e1a1-4434-e578-5396e8faddb3"
   },
   "outputs": [],
   "source": [
    "# Analysis on State column\n",
    "\n",
    "sale_count = df['State'].value_counts()\n",
    "sale_count_top10 = sale_count.head(10)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(sale_count_top10.index, sale_count_top10.values)\n",
    "plt.title('Number of Sales per state (top 10)')\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Number of Sales')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNumber of sales per state:\")\n",
    "display(sale_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pu351Dns9WOp"
   },
   "source": [
    "## Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "zRVefG0B0GWF",
    "outputId": "84bd5f56-ed3b-4831-d817-25ecb2be9dc7"
   },
   "outputs": [],
   "source": [
    "# Analysis of Catergory column\n",
    "\n",
    "category_counts = df['Category'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(category_counts, labels=category_counts.index, autopct='%1.1f%%', startangle=90, shadow=True)\n",
    "plt.title('Distribution of Product categories')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "Fw-RLHSj2xCY",
    "outputId": "923a3b4b-6517-40b1-90ca-9fede0f08df2"
   },
   "outputs": [],
   "source": [
    "# Plot to showcase various Sub-categories for each cateogries\n",
    "\n",
    "categories = df['Category'].unique()\n",
    "\n",
    "for category in categories:\n",
    "    category_df = df[df['Category'] == category]\n",
    "    subcategory_counts = category_df['Sub-Category'].value_counts()\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.pie(subcategory_counts, labels=subcategory_counts.index, autopct='%1.1f%%', startangle=90, shadow=True)\n",
    "    plt.title(f'Distribution of Subcategories in {category}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xa2ory69HSX"
   },
   "source": [
    "## Monthly Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "c6COOiG60Ftm",
    "outputId": "bf148bb7-baa2-4a4e-8600-19ac56d7f170"
   },
   "outputs": [],
   "source": [
    "# To visualise number of sales per month\n",
    "\n",
    "df['Order Month'] = df['Order Date'].dt.to_period('M')\n",
    "monthly_sales_count = df.groupby('Order Month').size().reset_index(name='Number of Sales')\n",
    "print(\"Number of sales per month:\")\n",
    "display(monthly_sales_count)\n",
    "\n",
    "# Plot for the same\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(monthly_sales_count['Order Month'].astype(str), monthly_sales_count['Number of Sales'])\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of Sales')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "XMRDp53I2w1z",
    "outputId": "e6c193c0-8d92-4845-f4c1-0dd9be8f4bf4"
   },
   "outputs": [],
   "source": [
    "# Montly data analysis\n",
    "\n",
    "monthly_category_sales = df.groupby([df['Order Date'].dt.to_period('M'), 'Category'])['Sales'].sum().reset_index()\n",
    "categories = monthly_category_sales['Category'].unique()\n",
    "\n",
    "plt.figure(figsize=(11, 5))\n",
    "\n",
    "for category in categories:\n",
    "    category_data = monthly_category_sales[monthly_category_sales['Category'] == category]\n",
    "    plt.plot(category_data['Order Date'].astype(str), category_data['Sales'], label=category)\n",
    "\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Sales')\n",
    "plt.grid(axis='y')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='Category')\n",
    "plt.tight_layout()\n",
    "plt.title(\"Montly Sales for each cateogries\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "JSQS8tHm2wvr",
    "outputId": "1eafb324-2bdb-4290-f511-0533511ac88c"
   },
   "outputs": [],
   "source": [
    "# Montly Profit for each categories\n",
    "\n",
    "monthly_category_profit = df.groupby([df['Order Date'].dt.to_period('M'), 'Category'])['Profit'].sum().reset_index()\n",
    "\n",
    "plt.figure(figsize=(11,5))\n",
    "for category in categories:\n",
    "    category_data = monthly_category_profit[monthly_category_profit['Category'] == category]\n",
    "    plt.plot(category_data['Order Date'].astype(str), category_data['Profit'], label=category)\n",
    "\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Profit')\n",
    "plt.grid(axis='y')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='Category')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "nyh40OBP2wcP",
    "outputId": "1d6425fb-c9f4-47ce-c081-d18e0175cb5e"
   },
   "outputs": [],
   "source": [
    "# Monthly trends\n",
    "monthly_sales = df.groupby('Order Month')['Sales'].sum().reset_index()\n",
    "monthly_sales_count = df.groupby('Order Month').size().reset_index(name='Number of Sales')\n",
    "monthly_trends = pd.merge(monthly_sales, monthly_sales_count, on='Order Month')\n",
    "print(monthly_trends)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ax1.plot(monthly_trends['Order Month'].astype(str), monthly_trends['Sales'], color='blue', label='Monthly Sales')\n",
    "ax1.set_xlabel('Month')\n",
    "ax1.set_ylabel('Total Sales')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "ax1.grid(axis='y')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(monthly_trends['Order Month'].astype(str), monthly_trends['Number of Sales'], color='red', label='Number of Sales')\n",
    "ax2.set_ylabel('Number of Sales')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rbyrsc3S9a2J"
   },
   "source": [
    "## Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "YlttER_H2w7o",
    "outputId": "6f0fec0c-6406-4ec6-b63a-43766cdc7793"
   },
   "outputs": [],
   "source": [
    "# Analysis on various Products in our dataset\n",
    "product_sales = df.groupby(['Product ID', 'Product Name', 'Category', 'Sub-Category'])['Quantity'].sum().reset_index()\n",
    "product_sales = product_sales.rename(columns={'Quantity': 'Number Sold'})\n",
    "\n",
    "productssortedsales = product_sales.sort_values(by='Number Sold', ascending=False)\n",
    "\n",
    "print(\"\\nProducts most bought:\")\n",
    "display(productssortedsales.head(10))\n",
    "\n",
    "print(\"\\nProducts least bought:\")\n",
    "display(productssortedsales.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "4joXuzmW4o8e",
    "outputId": "3fc78d72-7b18-43aa-ab6b-83362039c4bf"
   },
   "outputs": [],
   "source": [
    "# Product sales analysis\n",
    "\n",
    "product_sales = (df.groupby(['Product ID', 'Product Name'])['Sales'].sum().reset_index().sort_values(by='Sales', ascending=False).reset_index())\n",
    "display(product_sales.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blSUhw3f-S0W"
   },
   "source": [
    "## Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "PQ0fjVhe2wgo",
    "outputId": "7d5918b8-ef85-4e59-c28f-553a4c84c816"
   },
   "outputs": [],
   "source": [
    "# Regional data analysis\n",
    "\n",
    "regional_profit = df.groupby('Region')['Profit'].sum().reset_index()\n",
    "regional_profit = regional_profit.sort_values(by='Profit', ascending=False)\n",
    "\n",
    "print(\"Total profit by region:\")\n",
    "print(regional_profit)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(regional_profit['Region'], regional_profit['Profit'])\n",
    "plt.xlabel('Region')\n",
    "plt.ylabel('Total Profit')\n",
    "plt.title('Regional Profit Performance')\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ha2rxC6p7c9Q"
   },
   "source": [
    "# **PARETO RULE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_5VgWaF-t0a"
   },
   "source": [
    "## Pareto for Product Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "bBsf8Cli2wLf",
    "outputId": "c50a84e3-7c17-42b2-976b-0097d31f0b90"
   },
   "outputs": [],
   "source": [
    "# Pareto Principle for Product Sales\n",
    "\n",
    "product_sales['Cumulative Sales'] = product_sales['Sales'].cumsum()\n",
    "product_sales['Cumulative percent'] = (product_sales['Cumulative Sales'] / (product_sales['Sales'].sum())) * 100\n",
    "products_80 = (product_sales['Cumulative percent'] <= 80).sum() + 1\n",
    "\n",
    "print(f\"{products_80} products makes\")\n",
    "print(f\"{product_sales.loc[products_80-1,'Cumulative Sales']/product_sales['Sales'].sum()} of sales\")\n",
    "\n",
    "print(f\"\\n{products_80} out of {len(product_sales)}\")\n",
    "print(f\"that is {(products_80 / len(product_sales))*100}% of all products\")\n",
    "\n",
    "# Pareto Plot for Product Sales\n",
    "plt.figure(figsize=(10, 5))\n",
    "top80 = product_sales.head(products_80)\n",
    "plt.bar(range(1, len(product_sales) + 1), product_sales['Cumulative Sales'], width=1)\n",
    "plt.title('All products')\n",
    "plt.xlabel('Product sorted by sales)')\n",
    "\n",
    "plt.axhline(product_sales.loc[products_80-1,'Cumulative Sales'], color='yellow', linestyle='--', label='80% of sales')\n",
    "plt.axvline(products_80, color='red', linestyle='--', label='22.3% of products')\n",
    "\n",
    "plt.xticks(range(1, len(product_sales)+2, 100))\n",
    "plt.ylabel('Cumulative Sales')\n",
    "plt.ticklabel_format(style='plain', axis='y')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Actual Sales vs Pareto Principle plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(product_sales) + 1), product_sales['Cumulative percent'])\n",
    "plt.title('Pareto Rule(rule vs reality)')\n",
    "plt.xlabel('Product sorted by sales)')\n",
    "plt.ylabel('Cumulative percent of Sales')\n",
    "\n",
    "plt.axhline(80, color='green', linestyle='--', label='80% of Sales')\n",
    "plt.axvline(0.2 * len(product_sales), color='red', linestyle='--', label='20% of Products')\n",
    "plt.text(0.2 * len(product_sales), +5, f'{0.2 * len(product_sales):.0f}', color='red',ha = 'right')\n",
    "\n",
    "plt.axvline(products_80, color='green', linestyle='--', label=f'{(products_80 / len(product_sales))*100:.2f}% of Products')\n",
    "plt.text(products_80, +5, f'{products_80}', color='green')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(axis='y')\n",
    "plt.xticks(range(1, len(product_sales) + 1, 40), rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eNDngGzn-yBF"
   },
   "source": [
    "## Pareto for Customer Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "MjG_4ntM58nV",
    "outputId": "955d7cda-74ea-4b75-ad9e-cc0e5edaa748"
   },
   "outputs": [],
   "source": [
    "# Pareto principle for Customer Sales\n",
    "\n",
    "customer_sales = (df.groupby(['Customer ID', 'Customer Name'])['Sales'].sum().reset_index().sort_values(by='Sales', ascending=False)).reset_index()\n",
    "customer_sales['Cumulative Sales'] = customer_sales['Sales'].cumsum()\n",
    "customer_sales['Cumulative percent'] = (customer_sales['Cumulative Sales'] / (customer_sales['Sales'].sum()))*100\n",
    "customers_80 = (customer_sales['Cumulative percent'] <= 80).sum() + 1\n",
    "\n",
    "print((customer_sales[customer_sales['Cumulative percent'] <= 80]))\n",
    "print(customer_sales.loc[customers_80,'Cumulative Sales'])\n",
    "print(customer_sales['Sales'].sum())\n",
    "\n",
    "print(f\"\\n{customers_80} customers contribute to\")\n",
    "print(f\"{(customer_sales.loc[customers_80,'Cumulative Sales']/customer_sales['Sales'].sum())*100} of sales\")\n",
    "\n",
    "print(f\"\\n{customers_80} out of {len(customer_sales)}\")\n",
    "print(f\"that is {(customers_80 / len(customer_sales))*100}% of all customers\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "top80 = customer_sales.head(customers_80)\n",
    "plt.bar(range(1, len(customer_sales)+1 ), customer_sales['Cumulative Sales'],width=1)\n",
    "plt.title('All customers')\n",
    "plt.xlabel('customers sorted by sales)')\n",
    "\n",
    "plt.axhline(customer_sales.loc[customers_80,'Cumulative Sales'], color='yellow', linestyle='--', label='80% of sales')\n",
    "plt.axvline(customers_80, color='red', linestyle='--', label=f'{customers_80 / len(customer_sales) * 100:.1f}% of customers')\n",
    "\n",
    "plt.xticks(range(1, len(customer_sales)+2, 50))\n",
    "plt.ylabel('Cumulative Sales')\n",
    "plt.ticklabel_format(style='plain', axis='y')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Actual Sales Vs Pareto Principle for Customer Sales\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(customer_sales) + 1), customer_sales['Cumulative percent'])\n",
    "plt.title('Pareto Rule(rule vs reality)')\n",
    "plt.xlabel('Customer sorted by sales')\n",
    "plt.ylabel('Cumulative percent of Sales')\n",
    "\n",
    "plt.axhline(80, color='green', linestyle='--', label='80% of Sales')\n",
    "plt.axvline(0.2 * len(customer_sales), color='red', linestyle='--', label='20% of Customers')\n",
    "plt.text(0.2 * len(customer_sales), +5, f'{0.2 * len(customer_sales):.0f}', color='red')\n",
    "\n",
    "plt.axvline(customers_80, color='green', linestyle='--', label=f\"{(customers_80 / len(customer_sales))*100:.2f} % of Customers\")\n",
    "plt.text(customers_80, +5, f'{customers_80}', color='green')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(axis='y')\n",
    "plt.xticks(range(1, len(customer_sales) + 1, 100), rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2difs3NPRBtA"
   },
   "source": [
    "# **PREDICTING WITH PROFIT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odSDB1g2-40T"
   },
   "source": [
    "## **DATA PREPROCESSING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1RWzjTi_HsYf"
   },
   "source": [
    "### Train and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LJSY2Otj58g3",
    "outputId": "e8a12a35-de8c-46fb-a9a1-011e377f84e1"
   },
   "outputs": [],
   "source": [
    "# Splitting data into train and test set\n",
    "\n",
    "y = np.log1p(data['Sales'])   # Target variable: Sales\n",
    "X = data.copy()   # Feature dataset\n",
    "\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=50)\n",
    "\n",
    "print(f\"X_train_raw shape: {X_train_raw.shape}\")\n",
    "print(f\"X_test_raw shape: {X_test_raw.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vU8hBXexH1YN"
   },
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "6VvORhtn58a4",
    "outputId": "6f80ddfe-aa8a-45f4-8a4d-12ecfd947100"
   },
   "outputs": [],
   "source": [
    "def feature_engineer(X_train, X_test):\n",
    "\n",
    "    X_train_fe = X_train.copy()\n",
    "    X_test_fe = X_test.copy()\n",
    "\n",
    "    # Creating new features from our original features\n",
    "\n",
    "    # Product-level features\n",
    "    product_avg_quantity = X_train_fe.groupby('Product ID')['Quantity'].mean()\n",
    "    product_avg_discount = X_train_fe.groupby('Product ID')['Discount'].mean()\n",
    "    product_popularity = X_train_fe.groupby('Product ID')['Quantity'].sum()\n",
    "    product_avg_profit = X_train_fe.groupby('Product ID')['Profit'].mean()\n",
    "\n",
    "    # Customer-level features\n",
    "    customer_avg_quantity = X_train_fe.groupby('Customer ID')['Quantity'].mean()\n",
    "    customer_total_items = X_train_fe.groupby('Customer ID')['Quantity'].sum()\n",
    "    customer_avg_discount = X_train_fe.groupby('Customer ID')['Discount'].mean()\n",
    "    customer_avg_profit = X_train_fe.groupby('Customer ID')['Profit'].mean()\n",
    "\n",
    "    # Location-level features\n",
    "    city_avg_quantity = X_train_fe.groupby('City')['Quantity'].mean()\n",
    "    state_avg_discount = X_train_fe.groupby('State')['Discount'].mean()\n",
    "    city_avg_profit = X_train_fe.groupby('City')['Profit'].mean()\n",
    "    state_avg_profit = X_train_fe.groupby('State')['Profit'].mean()\n",
    "\n",
    "\n",
    "    # SubCategory-level features\n",
    "    subcategory_avg_quantity = X_train_fe.groupby('Sub-Category')['Quantity'].mean()\n",
    "    subcategory_avg_profit = X_train_fe.groupby('Sub-Category')['Profit'].mean()\n",
    "\n",
    "    # Category-level features\n",
    "    category_avg_profit = X_train_fe.groupby('Category')['Profit'].mean()\n",
    "\n",
    "\n",
    "    # Handling missing values\n",
    "    # Creating global values to replace missing values\n",
    "    global_avg_quantity = X_train_fe['Quantity'].mean()\n",
    "    global_avg_discount = X_train_fe['Discount'].mean()\n",
    "    global_avg_profit = X_train_fe['Profit'].mean()\n",
    "\n",
    "    # Creating df and mappings to handle the missing values\n",
    "    dataframes = [X_train_fe, X_test_fe]\n",
    "    mappings = {\n",
    "        'Product_Avg_Quantity': (product_avg_quantity, global_avg_quantity),\n",
    "        'Product_Avg_Discount': (product_avg_discount, global_avg_discount),\n",
    "        'Product_Popularity': (product_popularity, 0),\n",
    "        'Product_Avg_Profit': (product_avg_profit, global_avg_profit),\n",
    "        'Customer_Avg_Quantity': (customer_avg_quantity, global_avg_quantity),\n",
    "        'Customer_Total_Items': (customer_total_items, 0),\n",
    "        'Customer_Avg_Discount': (customer_avg_discount, global_avg_discount),\n",
    "        'Customer_Avg_Profit': (customer_avg_profit, global_avg_profit),\n",
    "        'City_Avg_Quantity': (city_avg_quantity, global_avg_quantity),\n",
    "        'State_Avg_Discount': (state_avg_discount, global_avg_discount),\n",
    "        'City_Avg_Profit': (city_avg_profit, global_avg_profit),\n",
    "        'State_Avg_Profit': (state_avg_profit, global_avg_profit),\n",
    "        'SubCategory_Avg_Quantity': (subcategory_avg_quantity, global_avg_quantity),\n",
    "        'SubCategory_Avg_Profit': (subcategory_avg_profit, global_avg_profit),\n",
    "        'Category_Avg_Profit': (category_avg_profit, global_avg_profit)\n",
    "    }\n",
    "\n",
    "    key_map = {\n",
    "        'Product_Avg_Quantity': 'Product ID', 'Product_Avg_Discount': 'Product ID', 'Product_Popularity': 'Product ID', 'Product_Avg_Profit': 'Product ID', # Added profit key\n",
    "        'Customer_Avg_Quantity': 'Customer ID', 'Customer_Total_Items': 'Customer ID', 'Customer_Avg_Discount': 'Customer ID', 'Customer_Avg_Profit': 'Customer ID', # Added profit key\n",
    "        'City_Avg_Quantity': 'City', 'State_Avg_Discount': 'State', 'City_Avg_Profit': 'City', 'State_Avg_Profit': 'State', # Added profit key\n",
    "        'SubCategory_Avg_Quantity': 'Sub-Category', 'SubCategory_Avg_Profit': 'Sub-Category',\n",
    "        'Category_Avg_Profit': 'Category'\n",
    "    }\n",
    "\n",
    "    for df in dataframes:\n",
    "        for new_col, (mapper, fill_val) in mappings.items():\n",
    "            map_key = key_map[new_col]\n",
    "            df[new_col] = df[map_key].map(mapper).fillna(fill_val)\n",
    "\n",
    "    # Creating more features\n",
    "    for df in dataframes:\n",
    "        # Date features\n",
    "        df['Order Date'] = pd.to_datetime(df['Order Date'], format='%m/%d/%Y')\n",
    "        df['Ship Date'] = pd.to_datetime(df['Ship Date'], format='%m/%d/%Y')\n",
    "        df['Order Year'] = df['Order Date'].dt.year.astype(str)\n",
    "        df['Order DayOfWeek'] = df['Order Date'].dt.dayofweek\n",
    "        df['Order Month'] = df['Order Date'].dt.month\n",
    "        df['Shipping Time'] = (df['Ship Date'] - df['Order Date']).dt.days\n",
    "        df['Is Weekend'] = df['Order DayOfWeek'].isin([5, 6]).astype(int)\n",
    "\n",
    "        # Cyclic features for dates to minimise the numerical value of dates, months and days\n",
    "        df['Month Sin'] = np.sin(2 * np.pi * df['Order Month'] / 12)\n",
    "        df['Month Cos'] = np.cos(2 * np.pi * df['Order Month'] / 12)\n",
    "        df['Day Sin'] = np.sin(2 * np.pi * df['Order DayOfWeek'] / 7)\n",
    "        df['Day Cos'] = np.cos(2 * np.pi * df['Order DayOfWeek'] / 7)\n",
    "\n",
    "        # Discount features\n",
    "        df['Is Discounted'] = (df['Discount'] > 0).astype(int)\n",
    "        df['Discount X Quantity'] = df['Discount'] * df['Quantity']\n",
    "\n",
    "        # Profit features # Added profit features\n",
    "        df['Profit Margin'] = df['Profit'] / (df['Sales'] + 1e-6)\n",
    "        df['Profit X Quantity'] = df['Profit'] * df['Quantity']\n",
    "\n",
    "\n",
    "    # Dropping unnecessary columns that include unique identifiers, original columns before feature extraction\n",
    "    cols_to_drop = [\n",
    "        'Row ID', 'Order ID', 'Customer Name', 'Country', 'Postal Code',\n",
    "        'Product Name', 'Sales','Profit',\n",
    "        'Order Date', 'Ship Date', 'Customer ID', 'Product ID',\n",
    "        'City', 'State', 'Order Month', 'Order DayOfWeek'\n",
    "    ]\n",
    "\n",
    "    X_train_fe = X_train_fe.drop(columns=cols_to_drop, errors='ignore')\n",
    "    X_test_fe = X_test_fe.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "    return X_train_fe, X_test_fe\n",
    "\n",
    "X_train, X_test = feature_engineer(X_train_raw, X_test_raw)\n",
    "\n",
    "print(f\"X_train shape after FE: {X_train.shape} \\n\")\n",
    "display(X_train.head())\n",
    "\n",
    "print(f\"X_test shape after FE: {X_test.shape} \\n\")\n",
    "display(X_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSIZ3iTrRrqG"
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eD-wYZij58Tx",
    "outputId": "594a9a6f-d8e3-45cf-dc68-bd67f7d98065"
   },
   "outputs": [],
   "source": [
    "def create_preprocessor(X_df):\n",
    "\n",
    "    # Identify feature types\n",
    "    categorical_features = list(X_df.select_dtypes(include=['object']).columns)\n",
    "    non_categorical_features = X_df.select_dtypes(exclude=['object']).columns\n",
    "    date_features = [col for col in non_categorical_features if col in ['Day Sin', 'Day Cos', 'Month Sin', 'Month Cos']]\n",
    "    binary_features = [col for col in non_categorical_features if col in ['Is Discounted', 'Is Weekend']]\n",
    "    numeric_features = [\n",
    "        col for col in non_categorical_features\n",
    "        if col not in date_features and col not in binary_features]\n",
    "\n",
    "    print(f\"\\nIdentified {len(numeric_features)} numeric features: {numeric_features}\")\n",
    "    print(f\"Identified {len(categorical_features)} categorical features: {categorical_features}\")\n",
    "\n",
    "    # Preprocessing pipelines\n",
    "\n",
    "    # Capping outliers using IQR and Power Transformation for handling the skewness of data\n",
    "    def cap_outliers(X_df_in, q_low=0.01, q_high=0.99):\n",
    "        X_capped = X_df_in.copy()\n",
    "        for col in X_capped.columns:\n",
    "            if X_capped[col].std() > 0:\n",
    "                low = X_capped[col].quantile(q_low)\n",
    "                high = X_capped[col].quantile(q_high)\n",
    "                X_capped[col] = np.clip(X_capped[col], low, high)\n",
    "        return X_capped\n",
    "\n",
    "    outlier_capper = FunctionTransformer(cap_outliers, validate=False)\n",
    "\n",
    "    numeric_pipeline = Pipeline(steps=[\n",
    "        ('cap_outliers', outlier_capper),\n",
    "        ('power_transform', PowerTransformer(method='yeo-johnson'))\n",
    "    ])\n",
    "\n",
    "    # ColumnTransformer to allow preprocessing to columns\n",
    "    transformers = []\n",
    "    if numeric_features:\n",
    "        transformers.append(('num', numeric_pipeline, numeric_features))\n",
    "    if date_features:\n",
    "        transformers.append(('date', 'passthrough', date_features))\n",
    "    if binary_features:\n",
    "        transformers.append(('binary', 'passthrough', binary_features))\n",
    "    if categorical_features:\n",
    "        transformers.append(('cat', OneHotEncoder(handle_unknown='ignore', sparse=False), categorical_features))\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=transformers,\n",
    "        remainder='drop'\n",
    "    )\n",
    "\n",
    "    return preprocessor, numeric_features, date_features, binary_features, categorical_features\n",
    "\n",
    "preprocessor, numeric_features, date_features, binary_features, categorical_features = create_preprocessor(X_train)\n",
    "\n",
    "print(\"Preprocessor created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qPArScDYTY3v"
   },
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "LcN5ZRj658Dk",
    "outputId": "9b54aebd-8ce5-45cc-c3f0-619f2bbdb75a"
   },
   "outputs": [],
   "source": [
    "# Create a temporary pipeline to get feature importances\n",
    "temp_rf_pipe = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=50, n_jobs=-1))\n",
    "])\n",
    "\n",
    "print(\"Fitting temporary RandomForest to find feature importances...\")\n",
    "temp_rf_pipe.fit(X_train, y_train)\n",
    "print(\"Done.\")\n",
    "\n",
    "# Get feature names from the ColumnTransformer\n",
    "numeric_pipe = preprocessor.named_transformers_['num']\n",
    "numeric_pipe.named_steps['cap_outliers'].feature_names_out = 'one-to-one'\n",
    "\n",
    "cat_features_ohe = list(preprocessor.named_transformers_['cat'].get_feature_names(categorical_features))\n",
    "all_features = numeric_features + date_features + binary_features + cat_features_ohe\n",
    "\n",
    "# Feature importances from the trained model\n",
    "importances = temp_rf_pipe.named_steps['regressor'].feature_importances_\n",
    "feature_importance_df = pd.DataFrame({'Feature': all_features, 'Importance': importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Selecting only the top 30 features and training on them\n",
    "top_features_ohe = list(feature_importance_df.head(30)['Feature'])\n",
    "\n",
    "print(\"\\n--- Top 10 Most Important Features ---\")\n",
    "display(feature_importance_df.head(10))\n",
    "\n",
    "# Finding which of the original features are in the top list\n",
    "top_numeric_date_binary_features = [\n",
    "    col for col in (numeric_features + date_features + binary_features)\n",
    "    if col in top_features_ohe]\n",
    "\n",
    "top_cat_features = [\n",
    "    col for col in categorical_features\n",
    "    if any(f.startswith(f\"{col}_\") for f in top_features_ohe)]\n",
    "\n",
    "final_feature_list = top_numeric_date_binary_features + top_cat_features\n",
    "print(f\"\\nUsing these {len(final_feature_list)} features for final models:\\n{final_feature_list}\")\n",
    "\n",
    "# New train and test set with new final features\n",
    "X_train_selected = X_train[final_feature_list]\n",
    "X_test_selected = X_test[final_feature_list]\n",
    "\n",
    "print(f\"\\nShape of X_train_selected: {X_train_selected.shape}\")\n",
    "print(f\"Shape of X_test_selected: {X_test_selected.shape}\")\n",
    "\n",
    "# Creating new preprocessor with new final features\n",
    "preprocessor_selected, _, _, _, _ = create_preprocessor(X_train_selected)\n",
    "print(\"\\nFinal 'preprocessor_selected' created using top features.\")\n",
    "\n",
    "X_train_selected = X_train\n",
    "X_test_selected = X_test\n",
    "preprocessor_selected = preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dH00zluZX3Yp"
   },
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Cya9BqKX5jS",
    "outputId": "f9228632-3c8f-4a90-e4d5-f5de132b4773"
   },
   "outputs": [],
   "source": [
    "# Building models of Linear Regression, Random Forest and XGBoost\n",
    "lr_reg = LinearRegression()\n",
    "rf_reg = RandomForestRegressor(random_state=50, n_jobs=-1)\n",
    "xgb_reg = XGBRegressor(random_state=50, n_jobs=-1, early_stopping_rounds=10)\n",
    "\n",
    "# Creating pipelines for each model\n",
    "lr_pipe = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_selected),\n",
    "    ('model', lr_reg)\n",
    "])\n",
    "xgb_pipe = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_selected),\n",
    "    ('model', xgb_reg)\n",
    "])\n",
    "\n",
    "rf_pipe = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_selected),\n",
    "    ('model', rf_reg)\n",
    "])\n",
    "\n",
    "# Defining parameter grids\n",
    "param_grid_lr = {}\n",
    "param_grid_xgb = {\n",
    "    'model__max_depth': randint(3, 7),\n",
    "    'model__min_child_weight': [1, 5, 10, 20],\n",
    "    'model__gamma': uniform(0, 0.5),\n",
    "    'model__reg_alpha': uniform(0, 1),\n",
    "    'model__reg_lambda': uniform(0, 1),\n",
    "    'model__n_estimators': randint(100, 500),\n",
    "    'model__learning_rate': uniform(0.01, 0.2),\n",
    "    'model__subsample': uniform(0.7, 0.3),\n",
    "    'model__colsample_bytree': uniform(0.7, 0.3)\n",
    "}\n",
    "\n",
    "param_grid_rf = {\n",
    "    'model__n_estimators': randint(100, 500),\n",
    "    'model__max_depth': randint(10, 30),\n",
    "    'model__min_samples_split': randint(2, 20),\n",
    "    'model__min_samples_leaf': randint(1, 10),\n",
    "    'model__max_features': ['sqrt', 'log2', 0.5, 0.7]\n",
    "}\n",
    "print(\"Models pipeline built successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "poV_AzHGZkve"
   },
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j-anCugMZsF7",
    "outputId": "a5189bb5-c5a9-4c82-eef2-7898d1c0f270"
   },
   "outputs": [],
   "source": [
    "# Creating validation set to reduce overfitting of XGBoost model\n",
    "X_train_tune, X_val_tune, y_train_tune, y_val_tune = train_test_split(\n",
    "    X_train_selected, y_train, test_size=0.2, random_state=50)\n",
    "\n",
    "print(f\"New tuning train set shape: {X_train_tune.shape}\")\n",
    "print(f\"New validation set shape: {X_val_tune.shape}\")\n",
    "\n",
    "print(\"Fitting preprocessor on new tuning data...\")\n",
    "preprocessor_fitted = preprocessor_selected.fit(X_train_tune)\n",
    "\n",
    "X_val_eval = preprocessor_fitted.transform(X_val_tune)\n",
    "print(\"Validation eval set created.\")\n",
    "\n",
    "fit_params_xgb = {\n",
    "    'model__eval_set': [(X_val_eval, y_val_tune)]\n",
    "}\n",
    "\n",
    "\n",
    "# Setting up RandomizedSearchCV\n",
    "\n",
    "lr_search = RandomizedSearchCV(\n",
    "    lr_pipe,\n",
    "    param_distributions=param_grid_lr,\n",
    "    n_iter=1,\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    random_state=50,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_search = RandomizedSearchCV(\n",
    "    xgb_pipe,\n",
    "    param_distributions=param_grid_xgb,\n",
    "    n_iter=50,\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    random_state=50,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_search = RandomizedSearchCV(\n",
    "    rf_pipe,\n",
    "    param_distributions=param_grid_rf,\n",
    "    n_iter=50,\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    random_state=50,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Tuning Linear Regression...\")\n",
    "lr_search.fit(X_train_tune, y_train_tune)\n",
    "\n",
    "print(\"Tuning XGBoost...\")\n",
    "xgb_search.fit(X_train_tune, y_train_tune, **fit_params_xgb)\n",
    "\n",
    "print(\"Tuning RandomForest...\")\n",
    "rf_search.fit(X_train_tune, y_train_tune)\n",
    "print(\"Tuning complete.\")\n",
    "\n",
    "print(f\"\\nBest Linear Regression CV R2 score: {lr_search.best_score_:.4f}\")\n",
    "print(f\"Best Linear Regression params: {lr_search.best_params_}\")\n",
    "print(f\"\\nBest XGBoost CV R2 score: {xgb_search.best_score_:.4f}\")\n",
    "print(f\"Best XGBoost params: {xgb_search.best_params_}\")\n",
    "print(f\"\\nBest RandomForest CV R2 score: {rf_search.best_score_:.4f}\")\n",
    "print(f\"Best RandomForest params: {rf_search.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q5mYMaQVdj44"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "r9G_VCpsdlUg",
    "outputId": "55203f51-e7f9-4991-939f-b034c94d194e"
   },
   "outputs": [],
   "source": [
    "best_lr_model = lr_search.best_estimator_\n",
    "best_xgb_model = xgb_search.best_estimator_\n",
    "best_rf_model = rf_search.best_estimator_\n",
    "\n",
    "# To store all the final results\n",
    "final_results = {}\n",
    "\n",
    "models_to_evaluate = {\n",
    "    \"Linear Regression (Tuned)\": best_lr_model,\n",
    "    \"XGBoost (Tuned)\": best_xgb_model,\n",
    "    \"RandomForest (Tuned)\": best_rf_model\n",
    "}\n",
    "\n",
    "for name, model in models_to_evaluate.items():\n",
    "\n",
    "    # Evaluate on TRAINING set\n",
    "    y_train_pred_log = model.predict(X_train_selected)\n",
    "    y_train_true = np.expm1(y_train) # Convert true y back from log\n",
    "    y_train_pred = np.expm1(y_train_pred_log) # Convert predictions back from log\n",
    "\n",
    "    train_r2 = r2_score(y_train_true, y_train_pred)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train_true, y_train_pred))\n",
    "\n",
    "    # Evaluate on TEST set\n",
    "    y_test_pred_log = model.predict(X_test_selected)\n",
    "    y_test_true = np.expm1(y_test) # Convert true y back from log\n",
    "    y_test_pred = np.expm1(y_test_pred_log) # Convert predictions back from log\n",
    "\n",
    "    test_r2 = r2_score(y_test_true, y_test_pred)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test_true, y_test_pred))\n",
    "\n",
    "    final_results[name] = {\n",
    "        'Train R2': round(train_r2, 4),\n",
    "        'Train RMSE': round(train_rmse, 2),\n",
    "        'Test R2': round(test_r2, 4),\n",
    "        'Test RMSE': round(test_rmse, 2)\n",
    "    }\n",
    "\n",
    "final_results_df = pd.DataFrame.from_dict(final_results, orient='index')\n",
    "print(\"\\n--- Final Model Results ---\")\n",
    "display(final_results_df)\n",
    "\n",
    "# Plots for the models\n",
    "print(\"\\n--- Final Model Plots ---\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Bar plot for R2 scores (Train vs Test)\n",
    "final_results_df[['Train R2', 'Test R2']].plot(kind='bar', ax=axes[0])\n",
    "axes[0].set_title('Final Model R2 Scores (Train vs Test)')\n",
    "axes[0].set_ylabel('R2 Score')\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "axes[0].legend(title='Dataset')\n",
    "\n",
    "# Bar plot for RMSE scores (Train vs Test)\n",
    "final_results_df[['Train RMSE', 'Test RMSE']].plot(kind='bar', ax=axes[1], color=['lightcoral', 'salmon'])\n",
    "axes[1].set_title('Final Model RMSE Scores (Train vs Test)')\n",
    "axes[1].set_ylabel('RMSE')\n",
    "axes[1].tick_params(axis='x', rotation=0)\n",
    "axes[1].legend(title='Dataset')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8Nlxq0Kf08I"
   },
   "source": [
    "Here, we see most of the Profit related features have very high feature importance. And so all our models have performed well, this evaluation of our model is not entirely corect, because in this case, because Profit and Sales are directly related features, there is significant Data Leakage.\n",
    "So to prevent this from happening, we will run our models without profit and profit related featues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEOcq3V3gzEi"
   },
   "source": [
    "# **PREDICTING WITHOUT PROFIT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "-ZJ3qdQ2gSLQ",
    "outputId": "0f843e8f-c1b9-46d8-bec1-19e1a478e87e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, PowerTransformer, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from scipy.stats import randint, uniform\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"Imports complete.\")\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(r\"Sample - Superstore.csv\", encoding='latin1')\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "# Split the data into training and test sets FIRST\n",
    "y = np.log1p(data['Sales'])  # log-transform target\n",
    "X = data.copy()\n",
    "\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=50\n",
    ")\n",
    "print(f\"X_train_raw shape: {X_train_raw.shape}, X_test_raw shape: {X_test_raw.shape}\")\n",
    "\n",
    "# Refactored Feature Engineering (WITHOUT PROFIT)\n",
    "def feature_engineer_without_profit(X_train, X_test):\n",
    "\n",
    "    X_train_fe = X_train.copy()\n",
    "    X_test_fe = X_test.copy()\n",
    "\n",
    "    # Calculate Aggregates from TRAINING data ONLY (safe features)\n",
    "    product_avg_quantity = X_train_fe.groupby('Product ID')['Quantity'].mean()\n",
    "    product_avg_discount = X_train_fe.groupby('Product ID')['Discount'].mean()\n",
    "    product_popularity = X_train_fe.groupby('Product ID')['Quantity'].sum()\n",
    "\n",
    "    customer_avg_quantity = X_train_fe.groupby('Customer ID')['Quantity'].mean()\n",
    "    customer_total_items = X_train_fe.groupby('Customer ID')['Quantity'].sum()\n",
    "    customer_avg_discount = X_train_fe.groupby('Customer ID')['Discount'].mean()\n",
    "\n",
    "    city_avg_quantity = X_train_fe.groupby('City')['Quantity'].mean()\n",
    "    state_avg_discount = X_train_fe.groupby('State')['Discount'].mean()\n",
    "\n",
    "    subcategory_avg_quantity = X_train_fe.groupby('Sub-Category')['Quantity'].mean()\n",
    "\n",
    "    # Global means for filling missing values\n",
    "    global_avg_quantity = X_train_fe['Quantity'].mean()\n",
    "    global_avg_discount = X_train_fe['Discount'].mean()\n",
    "\n",
    "    # Map Aggregates to both Train and Test sets\n",
    "    dataframes = [X_train_fe, X_test_fe]\n",
    "    mappings = {\n",
    "        'Product_Avg_Quantity': (product_avg_quantity, global_avg_quantity),\n",
    "        'Product_Avg_Discount': (product_avg_discount, global_avg_discount),\n",
    "        'Product_Popularity': (product_popularity, 0),\n",
    "        'Customer_Avg_Quantity': (customer_avg_quantity, global_avg_quantity),\n",
    "        'Customer_Total_Items': (customer_total_items, 0),\n",
    "        'Customer_Avg_Discount': (customer_avg_discount, global_avg_discount),\n",
    "        'City_Avg_Quantity': (city_avg_quantity, global_avg_quantity),\n",
    "        'State_Avg_Discount': (state_avg_discount, global_avg_discount),\n",
    "        'SubCategory_Avg_Quantity': (subcategory_avg_quantity, global_avg_quantity)\n",
    "    }\n",
    "    key_map = {\n",
    "        'Product_Avg_Quantity': 'Product ID', 'Product_Avg_Discount': 'Product ID', 'Product_Popularity': 'Product ID',\n",
    "        'Customer_Avg_Quantity': 'Customer ID', 'Customer_Total_Items': 'Customer ID', 'Customer_Avg_Discount': 'Customer ID',\n",
    "        'City_Avg_Quantity': 'City', 'State_Avg_Discount': 'State', 'SubCategory_Avg_Quantity': 'Sub-Category'\n",
    "    }\n",
    "\n",
    "    for df in dataframes:\n",
    "        for new_col, (mapper, fill_val) in mappings.items():\n",
    "            map_key = key_map[new_col]\n",
    "            df[new_col] = df[map_key].map(mapper).fillna(fill_val)\n",
    "\n",
    "        # Date and Discount features (original teammate features)\n",
    "        df['Order Date'] = pd.to_datetime(df['Order Date'], format='%m/%d/%Y')\n",
    "        df['Ship Date'] = pd.to_datetime(df['Ship Date'], format='%m/%d/%Y')\n",
    "        df['Order Year'] = df['Order Date'].dt.year.astype(str)\n",
    "        df['Order DayOfWeek'] = df['Order Date'].dt.dayofweek\n",
    "        df['Order Month'] = df['Order Date'].dt.month\n",
    "        df['Shipping Time'] = (df['Ship Date'] - df['Order Date']).dt.days\n",
    "        df['Is Weekend'] = df['Order DayOfWeek'].isin([5, 6]).astype(int)\n",
    "        df['Month Sin'] = np.sin(2 * np.pi * df['Order Month'] / 12)\n",
    "        df['Month Cos'] = np.cos(2 * np.pi * df['Order Month'] / 12)\n",
    "        df['Day Sin'] = np.sin(2 * np.pi * df['Order DayOfWeek'] / 7)\n",
    "        df['Day Cos'] = np.cos(2 * np.pi * df['Order DayOfWeek'] / 7)\n",
    "        df['Is Discounted'] = (df['Discount'] > 0).astype(int)\n",
    "        df['Discount X Quantity'] = df['Discount'] * df['Quantity']\n",
    "\n",
    "    # Drop Unnecessary Columns (identifiers, original columns, and now Profit/Sales)\n",
    "    cols_to_drop = [\n",
    "        'Row ID', 'Order ID', 'Customer Name', 'Country', 'Postal Code',\n",
    "        'Product Name', 'Profit', 'Sales', # Explicitly drop Profit and Sales\n",
    "        'Order Date', 'Ship Date', 'Customer ID', 'Product ID',\n",
    "        'City', 'State', 'Order Month', 'Order DayOfWeek'\n",
    "    ]\n",
    "    X_train_fe = X_train_fe.drop(columns=cols_to_drop, errors='ignore')\n",
    "    X_test_fe = X_test_fe.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "    return X_train_fe, X_test_fe\n",
    "\n",
    "# Run the feature engineering function\n",
    "X_train, X_test = feature_engineer_without_profit(X_train_raw, X_test_raw)\n",
    "\n",
    "print(f\"X_train shape after FE (without Profit): {X_train.shape}\")\n",
    "print(f\"X_test shape after FE (without Profit): {X_test.shape}\")\n",
    "\n",
    "\n",
    "# Refactored Preprocessor Creation\n",
    "def create_preprocessor(X_df):\n",
    "    categorical_features = list(X_df.select_dtypes(include=['object']).columns)\n",
    "    non_categorical_features = X_df.select_dtypes(exclude=['object']).columns\n",
    "    date_features = [col for col in non_categorical_features if col in ['Day Sin', 'Day Cos', 'Month Sin', 'Month Cos']]\n",
    "    binary_features = [col for col in non_categorical_features if col in ['Is Discounted', 'Is Weekend']]\n",
    "    numeric_features = [col for col in non_categorical_features if col not in date_features and col not in binary_features]\n",
    "\n",
    "    def cap_outliers(X_df_in, q_low=0.01, q_high=0.99):\n",
    "        X_capped = X_df_in.copy()\n",
    "        for col in X_capped.columns:\n",
    "            if X_capped[col].std() > 0:\n",
    "                low = X_capped[col].quantile(q_low)\n",
    "                high = X_capped[col].quantile(q_high)\n",
    "                X_capped[col] = np.clip(X_capped[col], low, high)\n",
    "        return X_capped\n",
    "\n",
    "    numeric_pipeline = Pipeline(steps=[\n",
    "        ('cap_outliers', FunctionTransformer(cap_outliers, validate=False)),\n",
    "        ('power_transform', PowerTransformer(method='yeo-johnson'))\n",
    "    ])\n",
    "\n",
    "    transformers = []\n",
    "    if numeric_features: transformers.append(('num', numeric_pipeline, numeric_features))\n",
    "    if date_features: transformers.append(('date', 'passthrough', date_features))\n",
    "    if binary_features: transformers.append(('binary', 'passthrough', binary_features))\n",
    "    if categorical_features: transformers.append(('cat', OneHotEncoder(handle_unknown='ignore', sparse=False), categorical_features))\n",
    "\n",
    "    preprocessor = ColumnTransformer(transformers=transformers, remainder='drop')\n",
    "    return preprocessor, numeric_features, date_features, binary_features, categorical_features\n",
    "\n",
    "# Create the preprocessor object AND capture the feature lists\n",
    "preprocessor, numeric_features, date_features, binary_features, categorical_features = create_preprocessor(X_train)\n",
    "print(\"Preprocessor created successfully.\")\n",
    "\n",
    "\n",
    "# Refactored Feature Selection\n",
    "# Create a temporary pipeline to get feature importances\n",
    "temp_rf_pipe = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=50, n_jobs=-1))\n",
    "])\n",
    "\n",
    "print(\"Fitting temporary RandomForest to find feature importances...\")\n",
    "temp_rf_pipe.fit(X_train, y_train)\n",
    "print(\"Done.\")\n",
    "\n",
    "# Get feature names from the ColumnTransformer and their importances\n",
    "try:\n",
    "    cat_features_ohe = list(preprocessor.named_transformers_['cat'].get_feature_names(categorical_features))\n",
    "    all_features = numeric_features + date_features + binary_features + cat_features_ohe\n",
    "    importances = temp_rf_pipe.named_steps['regressor'].feature_importances_\n",
    "    feature_importance_df = pd.DataFrame({'Feature': all_features, 'Importance': importances})\n",
    "    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Select the Top 30 features (OHE features)\n",
    "    top_features_ohe = list(feature_importance_df.head(30)['Feature'])\n",
    "\n",
    "    print(\"\\n--- Top 10 Most Important Features (without Profit) ---\")\n",
    "    display(feature_importance_df.head(10))\n",
    "\n",
    "    # Get the original column names that correspond to these top features\n",
    "    top_numeric_date_binary_features = [col for col in (numeric_features + date_features + binary_features) if col in top_features_ohe]\n",
    "    top_cat_features = [col for col in categorical_features if any(f.startswith(f\"{col}_\") for f in top_features_ohe)]\n",
    "    final_feature_list = top_numeric_date_binary_features + top_cat_features\n",
    "\n",
    "    print(f\"\\nUsing these {len(final_feature_list)} features for final models:\\n{final_feature_list}\")\n",
    "\n",
    "    # Filter X_train and X_test to only include these columns\n",
    "    X_train_selected = X_train[final_feature_list]\n",
    "    X_test_selected = X_test[final_feature_list]\n",
    "\n",
    "    print(f\"\\nShape of X_train_selected: {X_train_selected.shape}\")\n",
    "    print(f\"Shape of X_test_selected: {X_test_selected.shape}\")\n",
    "\n",
    "    # Create the *final* preprocessor using only the selected features\n",
    "    preprocessor_selected, _, _, _, _ = create_preprocessor(X_train_selected)\n",
    "    print(\"\\nFinal 'preprocessor_selected' created using top features.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during feature selection: {e}\")\n",
    "    print(\"Defaulting to use all features. Skipping feature selection.\")\n",
    "    X_train_selected = X_train\n",
    "    X_test_selected = X_test\n",
    "    preprocessor_selected = preprocessor\n",
    "\n",
    "\n",
    "# Refactored Model Definition and Tuning\n",
    "# Define the base regressors\n",
    "lr_reg = LinearRegression()\n",
    "rf_reg = RandomForestRegressor(random_state=50, n_jobs=-1)\n",
    "xgb_reg = XGBRegressor(random_state=50, n_jobs=-1, early_stopping_rounds=10)\n",
    "\n",
    "# Create pipelines\n",
    "lr_pipe = Pipeline(steps=[('preprocessor', preprocessor_selected), ('model', lr_reg)])\n",
    "xgb_pipe = Pipeline(steps=[('preprocessor', preprocessor_selected), ('model', xgb_reg)])\n",
    "rf_pipe = Pipeline(steps=[('preprocessor', preprocessor_selected), ('model', rf_reg)])\n",
    "\n",
    "# Define parameter grids\n",
    "param_grid_lr = {}\n",
    "param_grid_xgb = {\n",
    "    'model__max_depth': randint(3, 7),\n",
    "    'model__min_child_weight': [1, 5, 10, 20],\n",
    "    'model__gamma': uniform(0, 0.5),\n",
    "    'model__reg_alpha': uniform(0, 1),\n",
    "    'model__reg_lambda': uniform(0, 1),\n",
    "    'model__n_estimators': randint(100, 500),\n",
    "    'model__learning_rate': uniform(0.01, 0.2),\n",
    "    'model__subsample': uniform(0.7, 0.3),\n",
    "    'model__colsample_bytree': uniform(0.7, 0.3)\n",
    "}\n",
    "param_grid_rf = {\n",
    "    'model__n_estimators': randint(100, 500),\n",
    "    'model__max_depth': randint(10, 30),\n",
    "    'model__min_samples_split': randint(2, 20),\n",
    "    'model__min_samples_leaf': randint(1, 10),\n",
    "    'model__max_features': ['sqrt', 'log2', 0.5, 0.7]\n",
    "}\n",
    "\n",
    "# Set up RandomizedSearchCV for all three models\n",
    "lr_search = RandomizedSearchCV(lr_pipe, param_distributions=param_grid_lr, n_iter=1, cv=5, scoring='r2', random_state=50, n_jobs=-1)\n",
    "xgb_search = RandomizedSearchCV(xgb_pipe, param_distributions=param_grid_xgb, n_iter=50, cv=5, scoring='r2', random_state=50, n_jobs=-1)\n",
    "rf_search = RandomizedSearchCV(rf_pipe, param_distributions=param_grid_rf, n_iter=50, cv=5, scoring='r2', random_state=50, n_jobs=-1)\n",
    "\n",
    "# Split X_train_selected for the eval_set for XGBoost\n",
    "X_train_tune, X_val_tune, y_train_tune, y_val_tune = train_test_split(X_train_selected, y_train, test_size=0.2, random_state=50)\n",
    "preprocessor_fitted = preprocessor_selected.fit(X_train_tune)\n",
    "X_val_eval = preprocessor_fitted.transform(X_val_tune)\n",
    "fit_params_xgb = {'model__eval_set': [(X_val_eval, y_val_tune)]}\n",
    "\n",
    "\n",
    "# 5. Run the tuning for all three models\n",
    "print(\"\\nTuning Linear Regression...\")\n",
    "lr_search.fit(X_train_tune, y_train_tune)\n",
    "\n",
    "print(\"\\nTuning XGBoost...\")\n",
    "xgb_search.fit(X_train_tune, y_train_tune, **fit_params_xgb)\n",
    "\n",
    "print(\"\\nTuning RandomForest...\")\n",
    "rf_search.fit(X_train_tune, y_train_tune)\n",
    "\n",
    "print(\"Tuning complete.\")\n",
    "\n",
    "print(f\"\\nBest Linear Regression CV R2 score: {lr_search.best_score_:.4f}\")\n",
    "print(f\"Best Linear Regression params: {lr_search.best_params_}\")\n",
    "print(f\"\\nBest XGBoost CV R2 score: {xgb_search.best_score_:.4f}\")\n",
    "print(f\"Best XGBoost params: {xgb_search.best_params_}\")\n",
    "print(f\"\\nBest RandomForest CV R2 score: {rf_search.best_score_:.4f}\")\n",
    "print(f\"Best RandomForest params: {rf_search.best_params_}\")\n",
    "\n",
    "\n",
    "# Refactored Final Evaluation\n",
    "# Get the best estimators from the search\n",
    "best_lr_model = lr_search.best_estimator_\n",
    "best_xgb_model = xgb_search.best_estimator_\n",
    "best_rf_model = rf_search.best_estimator_\n",
    "\n",
    "# Create a dictionary to store final results\n",
    "final_results = {}\n",
    "\n",
    "# Evaluate the best models\n",
    "models_to_evaluate = {\n",
    "    \"Linear Regression (Tuned)\": best_lr_model,\n",
    "    \"XGBoost (Tuned)\": best_xgb_model,\n",
    "    \"RandomForest (Tuned)\": best_rf_model\n",
    "}\n",
    "\n",
    "for name, model in models_to_evaluate.items():\n",
    "    y_train_pred_log = model.predict(X_train_selected)\n",
    "    y_test_pred_log = model.predict(X_test_selected)\n",
    "\n",
    "    y_train_true = np.expm1(y_train)\n",
    "    y_train_pred = np.expm1(y_train_pred_log)\n",
    "    y_test_true = np.expm1(y_test)\n",
    "    y_test_pred = np.expm1(y_test_pred_log)\n",
    "\n",
    "    final_results[name] = {\n",
    "        'Train R2': round(r2_score(y_train_true, y_train_pred), 4),\n",
    "        'Train RMSE': round(np.sqrt(mean_squared_error(y_train_true, y_train_pred)), 2),\n",
    "        'Test R2': round(r2_score(y_test_true, y_test_pred), 4),\n",
    "        'Test RMSE': round(np.sqrt(mean_squared_error(y_test_true, y_test_pred)), 2)\n",
    "    }\n",
    "\n",
    "final_results_df = pd.DataFrame.from_dict(final_results, orient='index').sort_values(by='Test R2', ascending=False)\n",
    "print(\"\\n--- Final Model Results (without Profit) ---\")\n",
    "display(final_results_df)\n",
    "\n",
    "\n",
    "# Refactored Plots\n",
    "print(\"\\n--- Final Model Plots (without Profit) ---\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "final_results_df[['Train R2', 'Test R2']].plot(kind='bar', ax=axes[0])\n",
    "axes[0].set_title('Final Model R2 Scores (Train vs Test)')\n",
    "axes[0].set_ylabel('R2 Score')\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "axes[0].legend(title='Dataset')\n",
    "\n",
    "final_results_df[['Train RMSE', 'Test RMSE']].plot(kind='bar', ax=axes[1], color=['lightcoral', 'salmon'])\n",
    "axes[1].set_title('Final Model RMSE Scores (Train vs Test)')\n",
    "axes[1].set_ylabel('RMSE')\n",
    "axes[1].tick_params(axis='x', rotation=0)\n",
    "axes[1].legend(title='Dataset')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8m5rJ0ymC7k"
   },
   "source": [
    "Since Random Forest and XGBoost has outperformed Linear Regression, we will consider the latter two as our base models and perform feature interpretation and data visualisation accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kqw7eKpemfGX"
   },
   "source": [
    "# **FEATURE INTERPRETATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HF2pOd_Cmtsw"
   },
   "source": [
    "## SHAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5-ch9j3Vmylf",
    "outputId": "27f1b448-2e95-4a98-9585-05830cfb6c28"
   },
   "outputs": [],
   "source": [
    "# Prepare Data and Models for SHAP\n",
    "import shap\n",
    "import warnings\n",
    "import pandas as pd\n",
    "# Assuming X_train_selected is available and is a pandas DataFrame\n",
    "\n",
    "print(\"SHAP imported successfully.\")\n",
    "\n",
    "# Extract the fitted preprocessor from one of the best models\n",
    "try:\n",
    "    preprocessor = best_xgb_model.named_steps['preprocessor']\n",
    "    print(\"Preprocessor extracted.\")\n",
    "except NameError:\n",
    "    print(\"Error: 'best_xgb_model' not found. Please run the tuning cells first.\")\n",
    "    raise # Re-raise to stop execution if model is missing\n",
    "\n",
    "rf_model_only = best_rf_model.named_steps['model']\n",
    "xgb_model_only = best_xgb_model.named_steps['model']\n",
    "print(\"Models extracted from pipelines.\")\n",
    "\n",
    "numeric_pipe = preprocessor.named_transformers_['num']\n",
    "# Applying the previous fix for the FunctionTransformer\n",
    "numeric_pipe.named_steps['cap_outliers'].get_feature_names = 'one-to-one'\n",
    "print(\"Applied fix to FunctionTransformer 'cap_outliers'.\")\n",
    "\n",
    "# --- START OF FIX: Manually get and combine feature names ---\n",
    "\n",
    "# 1. Define your ORIGINAL numeric column names. \n",
    "# Replace the placeholder below with the actual list of column names used in your 'num' pipeline.\n",
    "# Example assumption: all numeric columns in X_train_selected were used.\n",
    "numeric_cols = list(X_train_selected.select_dtypes(include=['number']).columns)\n",
    "\n",
    "# 2. Get feature names from the 'cat' (OneHotEncoder) transformer\n",
    "try:\n",
    "    # Try the newer method first (get_feature_names_out)\n",
    "    cat_features_out = preprocessor.named_transformers_['cat'].get_feature_names_out()\n",
    "except AttributeError:\n",
    "    # Fallback to the older method (get_feature_names)\n",
    "    cat_features_out = preprocessor.named_transformers_['cat'].get_feature_names()\n",
    "\n",
    "# 3. Combine the numeric (original names) and categorical (OHE names) feature lists\n",
    "feature_names = list(numeric_cols) + list(cat_features_out)\n",
    "\n",
    "print(f\"Found {len(feature_names)} features after processing.\")\n",
    "# --- END OF FIX ---\n",
    "\n",
    "\n",
    "# Create the processed DataFrames for SHAP\n",
    "print(\"Processing data for SHAP...\")\n",
    "X_train_processed = preprocessor.transform(X_train_selected)\n",
    "X_test_processed = preprocessor.transform(X_test_selected)\n",
    "\n",
    "# Convert to dense DataFrames\n",
    "if hasattr(X_train_processed, \"toarray\"):\n",
    "    X_train_processed_df = pd.DataFrame(X_train_processed.toarray(), columns=feature_names)\n",
    "    X_test_processed_df = pd.DataFrame(X_test_processed.toarray(), columns=feature_names)\n",
    "else:\n",
    "    X_train_processed_df = pd.DataFrame(X_train_processed, columns=feature_names)\n",
    "    X_test_processed_df = pd.DataFrame(X_test_processed, columns=feature_names)\n",
    "\n",
    "# Create background data and test sample\n",
    "background_data = shap.sample(X_train_processed_df, 100, random_state=50)\n",
    "\n",
    "X_test_sample = shap.sample(X_test_processed_df, 200, random_state=50)\n",
    "\n",
    "print(\"Data preparation for SHAP is complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yes6R9Y7m1RP"
   },
   "source": [
    "### For RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "5bbf9f16d5184a319b9c24c4d4245a12",
      "79781812c89d4b8fb05d1ed37575f259",
      "e0aedbe67b09404b943f31a5b4726cd6",
      "f06f3b24e5bf419ab62cd905712a30f4",
      "6de03e2c037741fba39df3a97a464f6c",
      "d75a2dd175a74c078ac58f80c24ce518",
      "181ccdd28fc846d281586c7d3aa42a31",
      "574524bd5af546ac8db523bd3982daa9",
      "4c33154b617b43109c76eb8ba8ca743d",
      "eb1ddbce18554c3787cd9fab791d5a8a",
      "71bdfbbea2114f238cd53726dd88c3af"
     ]
    },
    "id": "1sGxFi8amnvy",
    "outputId": "a4ee0068-9f9a-420e-cf3f-1b00149313a1"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# SHAP Analysis for RandomForest (KernelExplainer\n",
    "\n",
    "print(f\"--- SHAP Analysis for: RandomForest (Tuned) ---\")\n",
    "\n",
    "# Create the KernelExplainer\n",
    "print(\"Running KernelExplainer\")\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    rf_explainer = shap.KernelExplainer(rf_model_only.predict, background_data)\n",
    "\n",
    "# Calculate SHAP values for our test sample\n",
    "rf_shap_values = rf_explainer.shap_values(X_test_sample)\n",
    "print(\"SHAP values calculated.\")\n",
    "\n",
    "# Create a map of ugly feature names to pretty names for plotting\n",
    "feature_name_map = {\n",
    "    'num__Discount': 'Discount',\n",
    "    'num__Shipping Time': 'Shipping Time',\n",
    "    'num__Discount X Quantity': 'Discount x Quantity',\n",
    "    'num__Customer Total Items': 'Customer Total Items',\n",
    "    'num__Customer Avg Discount': 'Customer Avg Discount',\n",
    "    'num__Product Popularity': 'Product Popularity',\n",
    "    'num__Product Unique Customers': 'Product Unique Customers',\n",
    "    'date__Day Sin': 'Day Sin', 'date__Day Cos': 'Day Cos',\n",
    "    'date__Month Sin': 'Month Sin', 'date__Month Cos': 'Month Cos',\n",
    "    'binary__Is Discounted': 'Is Discounted',\n",
    "    'binary__Is Weekend': 'Is Weekend',\n",
    "    'cat__Ship Mode_First Class': 'Ship Mode: First Class',\n",
    "    'cat__Ship Mode_Same Day': 'Ship Mode: Same Day',\n",
    "    'cat__Ship Mode_Second Class': 'Ship Mode: Second Class',\n",
    "    'cat__Ship Mode_Standard Class': 'Ship Mode: Standard Class',\n",
    "    'cat__Segment_Consumer': 'Segment: Consumer',\n",
    "    'cat__Segment_Corporate': 'Segment: Corporate',\n",
    "    'cat__Segment_Home Office': 'Segment: Home Office',\n",
    "    'cat__Region_Central': 'Region: Central',\n",
    "    'cat__Region_East': 'Region: East',\n",
    "    'cat__Region_South': 'Region: South',\n",
    "    'cat__Region_West': 'Region: West',\n",
    "    'cat__Category_Furniture': 'Category: Furniture',\n",
    "    'cat__Category_Office Supplies': 'Category: Office Supplies',\n",
    "    'cat__Category_Technology': 'Category: Technology',\n",
    "    'cat__Sub-Category_Accessories': 'Sub-Category: Accessories',\n",
    "    'cat__Sub-Category_Appliances': 'Sub-Category: Appliances',\n",
    "    'cat__Sub-Category_Art': 'Sub-Category: Art',\n",
    "    'cat__Sub-Category_Binders': 'Sub-Category: Binders',\n",
    "    'cat__Sub-Category_Bookcases': 'Sub-Category: Bookcases',\n",
    "    'cat__Sub-Category_Chairs': 'Sub-Category: Chairs',\n",
    "    'cat__Sub-Category_Copiers': 'Sub-Category: Copiers',\n",
    "    'cat__Sub-Category_Envelopes': 'Sub-Category: Envelopes',\n",
    "    'cat__Sub-Category_Fasteners': 'Sub-Category: Fasteners',\n",
    "    'cat__Sub-Category_Furnishings': 'Sub-Category: Furnishings',\n",
    "    'cat__Sub-Category_Labels': 'Sub-Category: Labels',\n",
    "    'cat__Sub-Category_Machines': 'Sub-Category: Machines',\n",
    "    'cat__Sub-Category_Paper': 'Sub-Category: Paper',\n",
    "    'cat__Sub-Category_Phones': 'Sub-Category: Phones',\n",
    "    'cat__Sub-Category_Storage': 'Sub-Category: Storage',\n",
    "    'cat__Sub-Category_Supplies': 'Sub-Category: Supplies',\n",
    "    'cat__Sub-Category_Tables': 'Sub-Category: Tables',\n",
    "    'cat__Order Year_2014': 'Year: 2014',\n",
    "    'cat__Order Year_2015': 'Year: 2015',\n",
    "    'cat__Order Year_2016': 'Year: 2016',\n",
    "    'cat__Order Year_2017': 'Year: 2017'\n",
    "}\n",
    "\n",
    "# Create a copy of the test sample with renamed columns for plotting\n",
    "X_test_sample_renamed_rf = X_test_sample.copy()\n",
    "X_test_sample_renamed_rf.columns = X_test_sample_renamed_rf.columns.map(lambda x: feature_name_map.get(x, x))\n",
    "\n",
    "# Plot the Beeswarm Plot\n",
    "print(\"\\nBeeswarm Plot (Importance & Impact of Features):\")\n",
    "shap.summary_plot(rf_shap_values, X_test_sample_renamed_rf, plot_type=\"dot\", max_display=15, show=False)\n",
    "\n",
    "fig = plt.gcf()\n",
    "ax = plt.gca()\n",
    "plt.title(f\"SHAP Beeswarm Plot for RandomForest (Tuned)\", y=1.05)\n",
    "ax.set_xlabel(\"Impact on Sales Prediction (Helps <--> Hurts)\")\n",
    "cb = fig.axes[-1]\n",
    "cb.set_ylabel(\"Feature Value\")\n",
    "cb.set_yticks([ax.get_ylim()[0], ax.get_ylim()[1]])\n",
    "cb.set_yticklabels([\"Low\", \"High\"])\n",
    "plot_text = (\n",
    "    \"  Y-Axis = Feature Rank (Top = Most Important)\\n\"\n",
    "    \"  X-Axis = Impact (Right = Helps Sales, Left = Hurts Sales)\\n\"\n",
    "    \"  Color = Feature Value (Red = High Value, Blue = Low Value)\"\n",
    ")\n",
    "fig.text(0.13, -0.1, plot_text, horizontalalignment='left', verticalalignment='bottom', wrap=True)\n",
    "\n",
    "plt.savefig(\"rf_beeswarm_plot.png\", bbox_inches='tight')\n",
    "print(\"Saved rf_beeswarm_plot.png\")\n",
    "plt.show()\n",
    "\n",
    "# 6. Plot the Dependence Plot for 'Discount'\n",
    "print(f\"\\nDependence Plot for 'Discount':\")\n",
    "shap.dependence_plot(\n",
    "    'Discount',\n",
    "    rf_shap_values,\n",
    "    X_test_sample_renamed_rf,\n",
    "    interaction_index=None,\n",
    "    show=False\n",
    ")\n",
    "\n",
    "fig_dep = plt.gcf()\n",
    "ax_dep = plt.gca()\n",
    "plt.title(f\"Dependence Plot for 'Discount' (RandomForest Model)\", y=1.05)\n",
    "ax_dep.set_xlabel(\"Discount Feature Value\")\n",
    "ax_dep.set_ylabel(\"SHAP Value (Impact on Sales)\")\n",
    "dep_explanation = (\n",
    "    \"  X-Axis = The actual value of the Discount\\n\"\n",
    "    \"  Y-Axis = The impact on sales (Above 0 = Helps, Below 0 = Hurts)\"\n",
    ")\n",
    "fig_dep.text(0.13, -0.1, dep_explanation, horizontalalignment='left', verticalalignment='bottom', wrap=True)\n",
    "\n",
    "plt.savefig(\"rf_dependence_plot.png\", bbox_inches='tight')\n",
    "print(\"Saved rf_dependence_plot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpjLTdgznBxf"
   },
   "source": [
    "### For XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "bbc2e2864d08416a83fbec6d43cd0c83",
      "f7ac3f718d7d4259a5cf1cfc11680528",
      "52497735278941288115fc9f42d6a5bd",
      "521996eba56c4b16800f6e2b53af703f",
      "df7ed105bae04b92b791285046802819",
      "c04180644bd043eb91e5a236d5a2b2d2",
      "94047c5bc710410f9a439574a91086e4",
      "83faca0413e44c9a8a582605785f73a1",
      "4ef3f9db8dc14c3386297539e5a30e65",
      "f0f28f0c6dad4da0ad05872985b51d30",
      "0f51ad86f2a849eab83ad481d7c024c5"
     ]
    },
    "id": "GktslBj2nD_N",
    "outputId": "181ed931-e6ae-4ea7-e644-87b8558d7aaf"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# SHAP Analysis for XGBoost (KernelExplainer)\n",
    "print(f\"--- SHAP Analysis for: XGBoost (Tuned) ---\")\n",
    "\n",
    "# Create the KernelExplainer\n",
    "print(\"...Running KernelExplainer (this may take a few minutes)...\")\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    xgb_explainer = shap.KernelExplainer(xgb_model_only.predict, background_data)\n",
    "\n",
    "# Calculate SHAP values for our test sample\n",
    "xgb_shap_values = xgb_explainer.shap_values(X_test_sample)\n",
    "print(\"SHAP values calculated.\")\n",
    "\n",
    "# Create a map of ugly feature names to pretty names for plotting\n",
    "feature_name_map = {\n",
    "    'num__Discount': 'Discount',\n",
    "    'num__Shipping Time': 'Shipping Time',\n",
    "    'num__Discount X Quantity': 'Discount x Quantity',\n",
    "    'num__Customer Total Items': 'Customer Total Items',\n",
    "    'num__Customer Avg Discount': 'Customer Avg Discount',\n",
    "    'num__Product Popularity': 'Product Popularity',\n",
    "    'num__Product Unique Customers': 'Product Unique Customers',\n",
    "    'date__Day Sin': 'Day Sin', 'date__Day Cos': 'Day Cos',\n",
    "    'date__Month Sin': 'Month Sin', 'date__Month Cos': 'Month Cos',\n",
    "    'binary__Is Discounted': 'Is Discounted',\n",
    "    'binary__Is Weekend': 'Is Weekend',\n",
    "    'cat__Ship Mode_First Class': 'Ship Mode: First Class',\n",
    "    'cat__Ship Mode_Same Day': 'Ship Mode: Same Day',\n",
    "    'cat__Ship Mode_Second Class': 'Ship Mode: Second Class',\n",
    "    'cat__Ship Mode_Standard Class': 'Ship Mode: Standard Class',\n",
    "    'cat__Segment_Consumer': 'Segment: Consumer',\n",
    "    'cat__Segment_Corporate': 'Segment: Corporate',\n",
    "    'cat__Segment_Home Office': 'Segment: Home Office',\n",
    "    'cat__Region_Central': 'Region: Central',\n",
    "    'cat__Region_East': 'Region: East',\n",
    "    'cat__Region_South': 'Region: South',\n",
    "    'cat__Region_West': 'Region: West',\n",
    "    'cat__Category_Furniture': 'Category: Furniture',\n",
    "    'cat__Category_Office Supplies': 'Category: Office Supplies',\n",
    "    'cat__Category_Technology': 'Category: Technology',\n",
    "    'cat__Sub-Category_Accessories': 'Sub-Category: Accessories',\n",
    "    'cat__Sub-Category_Appliances': 'Sub-Category: Appliances',\n",
    "    'cat__Sub-Category_Art': 'Sub-Category: Art',\n",
    "    'cat__Sub-Category_Binders': 'Sub-Category: Binders',\n",
    "    'cat__Sub-Category_Bookcases': 'Sub-Category: Bookcases',\n",
    "    'cat__Sub-Category_Chairs': 'Sub-Category: Chairs',\n",
    "    'cat__Sub-Category_Copiers': 'Sub-Category: Copiers',\n",
    "    'cat__Sub-Category_Envelopes': 'Sub-Category: Envelopes',\n",
    "    'cat__Sub-Category_Fasteners': 'Sub-Category: Fasteners',\n",
    "    'cat__Sub-Category_Furnishings': 'Sub-Category: Furnishings',\n",
    "    'cat__Sub-Category_Labels': 'Sub-Category: Labels',\n",
    "    'cat__Sub-Category_Machines': 'Sub-Category: Machines',\n",
    "    'cat__Sub-Category_Paper': 'Sub-Category: Paper',\n",
    "    'cat__Sub-Category_Phones': 'Sub-Category: Phones',\n",
    "    'cat__Sub-Category_Storage': 'Sub-Category: Storage',\n",
    "    'cat__Sub-Category_Supplies': 'Sub-Category: Supplies',\n",
    "    'cat__Sub-Category_Tables': 'Sub-Category: Tables',\n",
    "    'cat__Order Year_2014': 'Year: 2014',\n",
    "    'cat__Order Year_2015': 'Year: 2015',\n",
    "    'cat__Order Year_2016': 'Year: 2016',\n",
    "    'cat__Order Year_2017': 'Year: 2017'\n",
    "}\n",
    "\n",
    "# Create a copy of the test sample with renamed columns for plotting\n",
    "X_test_sample_renamed_xgb = X_test_sample.copy()\n",
    "X_test_sample_renamed_xgb.columns = X_test_sample_renamed_xgb.columns.map(lambda x: feature_name_map.get(x, x))\n",
    "\n",
    "# Plot the Beeswarm Plot\n",
    "print(\"\\nBeeswarm Plot (Importance & Impact of Features):\")\n",
    "shap.summary_plot(xgb_shap_values, X_test_sample_renamed_xgb, plot_type=\"dot\", max_display=15, show=False)\n",
    "\n",
    "fig = plt.gcf()\n",
    "ax = plt.gca()\n",
    "plt.title(f\"SHAP Beeswarm Plot for XGBoost (Tuned)\", y=1.05)\n",
    "ax.set_xlabel(\"Impact on Sales Prediction (Helps <--> Hurts)\")\n",
    "cb = fig.axes[-1]\n",
    "cb.set_ylabel(\"Feature Value\")\n",
    "cb.set_yticks([ax.get_ylim()[0], ax.get_ylim()[1]])\n",
    "cb.set_yticklabels([\"Low\", \"High\"])\n",
    "plot_text = (\n",
    "    \"  Y-Axis = Feature Rank (Top = Most Important)\\n\"\n",
    "    \"  X-Axis = Impact (Right = Helps Sales, Left = Hurts Sales)\\n\"\n",
    "    \"  Color = Feature Value (Red = High Value, Blue = Low Value)\"\n",
    ")\n",
    "fig.text(0.13, -0.1, plot_text, horizontalalignment='left', verticalalignment='bottom', wrap=True)\n",
    "\n",
    "plt.savefig(\"xgb_beeswarm_plot.png\", bbox_inches='tight')\n",
    "print(\"Saved xgb_beeswarm_plot.png\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the Dependence Plot for 'Discount'\n",
    "print(f\"\\nDependence Plot for 'Discount':\")\n",
    "shap.dependence_plot(\n",
    "    'Discount',\n",
    "    xgb_shap_values,\n",
    "    X_test_sample_renamed_xgb,\n",
    "    interaction_index=None,\n",
    "    show=False\n",
    ")\n",
    "\n",
    "fig_dep = plt.gcf()\n",
    "ax_dep = plt.gca()\n",
    "plt.title(f\"Dependence Plot for 'Discount' (XGBoost Model)\", y=1.05)\n",
    "ax_dep.set_xlabel(\"Discount Feature Value\")\n",
    "ax_dep.set_ylabel(\"SHAP Value (Impact on Sales)\")\n",
    "dep_explanation = (\n",
    "    \"  X-Axis = The actual value of the Discount\\n\"\n",
    "    \"  Y-Axis = The impact on sales (Above 0 = Helps, Below 0 = Hurts)\"\n",
    ")\n",
    "fig_dep.text(0.13, -0.1, dep_explanation, horizontalalignment='left', verticalalignment='bottom', wrap=True)\n",
    "\n",
    "plt.savefig(\"xgb_dependence_plot.png\", bbox_inches='tight')\n",
    "print(\"Saved xgb_dependence_plot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rApv0xL-n3Vc"
   },
   "source": [
    "## Permutation Importance method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEs7mNG5CTc_"
   },
   "source": [
    "### XGBoost\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 777
    },
    "id": "Kh-FNGmYn7yo",
    "outputId": "81a7ec24-2a55-48a1-eefb-97e8a58c75f0"
   },
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "print(\"Calculating Permutation Importance...\")\n",
    "\n",
    "best_model = best_xgb_model\n",
    "X_test_perm = X_test_selected\n",
    "y_test_perm = y_test\n",
    "\n",
    "# Permutation importance\n",
    "perm_importance = permutation_importance(\n",
    "  best_model,\n",
    "  X_test_perm,\n",
    "  y_test_perm,\n",
    "  n_repeats=10,\n",
    "  random_state=50,\n",
    "  n_jobs=-1\n",
    ")\n",
    "\n",
    "# Store results in a DataFrame\n",
    "feature_names = X_test_perm.columns\n",
    "perm_importance_df = pd.DataFrame({\n",
    "  'Feature': feature_names,\n",
    "  'Importance_mean': perm_importance.importances_mean,\n",
    "  'Importance_std': perm_importance.importances_std\n",
    "})\n",
    "\n",
    "perm_importance_df = perm_importance_df.sort_values(by='Importance_mean', ascending=False)\n",
    "\n",
    "print(\"Permutation Importance Calculation Complete.\")\n",
    "\n",
    "print(\"\\n--- Permutation Importance Results (Mean Importance) ---\")\n",
    "display(perm_importance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "bmO10sIAA5aG",
    "outputId": "8d266708-dfdd-40a6-c67f-9aa25aff517f"
   },
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(perm_importance_df['Feature'], perm_importance_df['Importance_mean'], yerr=perm_importance_df['Importance_std'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Permutation Importance of Features (XGBoost Model)')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Mean Decrease in R2 (Importance)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNAKTiZIo4tt"
   },
   "source": [
    "### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8BZp35ihpNXH"
   },
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "print(\"Calculating Permutation Importance for Random Forest...\")\n",
    "\n",
    "best_model = best_rf_model\n",
    "X_test_perm = X_test_selected\n",
    "y_test_perm = y_test\n",
    "\n",
    "perm_importance = permutation_importance(\n",
    "    best_model,\n",
    "    X_test_perm,\n",
    "    y_test_perm,\n",
    "    n_repeats=10,\n",
    "    random_state=50,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Store results in a DataFrame\n",
    "feature_names = X_test_perm.columns\n",
    "perm_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance_mean': perm_importance.importances_mean,\n",
    "    'Importance_std': perm_importance.importances_std\n",
    "})\n",
    "\n",
    "perm_importance_df = perm_importance_df.sort_values(by='Importance_mean', ascending=False)\n",
    "\n",
    "print(\"Permutation Importance Calculation Complete for Random Forest.\")\n",
    "\n",
    "print(\"\\n--- Permutation Importance Results (Mean Importance) for Random Forest ---\")\n",
    "display(perm_importance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gbYsGaZZDDRr"
   },
   "outputs": [],
   "source": [
    " # Plot the results\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(perm_importance_df['Feature'], perm_importance_df['Importance_mean'], yerr=perm_importance_df['Importance_std'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Permutation Importance of Features (Random Forest Model)')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Mean Decrease in R2 (Importance)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pxHAaPFepO9N"
   },
   "source": [
    "#**DASHBOARD SUMMARY**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdcjqQcWpftB"
   },
   "source": [
    "## With Profit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qS7YBE1piku"
   },
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CfGvGNYgKF5E"
   },
   "outputs": [],
   "source": [
    "# Consolidating visualizations for Random Forest Model (With Profit)\n",
    "\n",
    "print(\"\\n--- Visualizations for Random Forest Model (With Profit) ---\")\n",
    "\n",
    "try:\n",
    "    rf_model = best_rf_model\n",
    "\n",
    "    y_test_pred_log_rf = rf_model.predict(X_test_selected)\n",
    "    y_test_true_rf = np.expm1(y_test)\n",
    "    y_test_pred_rf = np.expm1(y_test_pred_log_rf)\n",
    "\n",
    "    residuals_rf = y_test_true_rf - y_test_pred_rf\n",
    "\n",
    "    plot_data_rf = X_test_selected.copy()\n",
    "    plot_data_rf['Actual_Sales'] = y_test_true_rf\n",
    "    plot_data_rf['Predicted_Sales'] = y_test_pred_rf\n",
    "    plot_data_rf['Residuals'] = residuals_rf\n",
    "\n",
    "\n",
    "    if 'Product_Popularity' in plot_data_rf.columns:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(plot_data_rf['Product_Popularity'], plot_data_rf['Residuals'], alpha=0.5)\n",
    "        plt.axhline(y=0, color='r', linestyle='--')\n",
    "        plt.title('Random Forest (With Profit): Residuals vs. Product Popularity (Test Set)')\n",
    "        plt.xlabel('Product Popularity (Total Quantity)')\n",
    "        plt.ylabel('Residuals (Actual Sales - Predicted Sales)')\n",
    "        plt.grid(True)\n",
    "        print(\"\\nDisplaying rf_residuals_vs_popularity plot (With Profit).\")\n",
    "        plt.show()\n",
    "        print(\"\\n--- Inference: Residuals vs. Product Popularity ---\")\n",
    "        print(\"This plot shows if there is a pattern in the errors (residuals) of the model's predictions based on the popularity of a product.\")\n",
    "        print(\"Ideally, the points should be randomly scattered around the zero line.\")\n",
    "\n",
    "    if 'Category' in plot_data_rf.columns:\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        category_agg_rf = plot_data_rf.groupby('Category')[['Actual_Sales', 'Predicted_Sales']].mean().reset_index()\n",
    "        category_agg_rf.plot(x='Category', y=['Actual_Sales', 'Predicted_Sales'], kind='bar', ax=plt.gca())\n",
    "        plt.title('Random Forest (With Profit): Average Actual vs. Predicted Sales by Category (Test Set)')\n",
    "        plt.xlabel('Category')\n",
    "        plt.ylabel('Average Sales')\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.grid(axis='y')\n",
    "        plt.tight_layout()\n",
    "        print(\"\\nDisplaying rf_predicted_actual_by_category plot (With Profit).\")\n",
    "        plt.show()\n",
    "        print(\"\\n--- Inference: Predicted vs. Actual by Category ---\")\n",
    "        print(\"This bar plot compares the average actual sales to the average predicted sales for each product category.\")\n",
    "        print(\"Ideally, the 'Predicted_Sales' bar should be close to the 'Actual_Sales' bar.\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(y_test_pred_rf, bins=50, edgecolor='black')\n",
    "    plt.title('Random Forest (With Profit): Distribution of Predicted Sales Values (Test Set)')\n",
    "    plt.xlabel('Predicted Sales')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    print(\"\\nDisplaying rf_predicted_sales_distribution plot (With Profit).\")\n",
    "    plt.show()\n",
    "    print(\"\\n--- Inference: Distribution of Predicted Sales Values ---\")\n",
    "    print(\"This histogram shows the frequency distribution of the model's predicted sales values.\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test_pred_rf, residuals_rf, alpha=0.5)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.title('Random Forest (With Profit): Residuals vs. Predicted Values (Test Set)')\n",
    "    plt.xlabel('Predicted Sales')\n",
    "    plt.ylabel('Residuals (Actual Sales - Predicted Sales)')\n",
    "    plt.grid(True)\n",
    "    print(\"\\nDisplaying rf_residuals_vs_predicted plot (With Profit).\")\n",
    "    plt.show()\n",
    "    print(\"\\n--- Inference: Residuals vs. Predicted Values ---\")\n",
    "    print(\"Randomly scattered points around the zero line are desired. Patterns here (like a funnel shape) suggest heteroscedasticity.\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(residuals_rf, bins=50, edgecolor='black')\n",
    "    plt.axvline(x=0, color='r', linestyle='--')\n",
    "    plt.title('Random Forest (With Profit): Distribution of Residuals (Test Set)')\n",
    "    plt.xlabel('Residuals (Actual Sales - Predicted Sales)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    print(\"\\nDisplaying rf_residuals_distribution plot (With Profit).\")\n",
    "    plt.show()\n",
    "    print(\"\\n--- Inference: Distribution of Residuals ---\")\n",
    "    print(\"For a good model, the residuals should ideally be normally distributed around zero.\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test_true_rf, y_test_pred_rf, alpha=0.5)\n",
    "    plt.plot([y_test_true_rf.min(), y_test_true_rf.max()], [y_test_true_rf.min(), y_test_true_rf.max()], 'r--', lw=2)\n",
    "    plt.title('Random Forest (With Profit): Predicted vs. Actual Sales (Test Set)')\n",
    "    plt.xlabel('Actual Sales')\n",
    "    plt.ylabel('Predicted Sales')\n",
    "    plt.grid(True)\n",
    "    print(\"\\nDisplaying rf_predicted_actual_scatter plot (With Profit).\")\n",
    "    plt.show()\n",
    "    print(\"\\n--- Inference: Predicted vs. Actual Sales Scatter Plot ---\")\n",
    "    print(\"Points should ideally lie close to the red dashed line.\")\n",
    "\n",
    "    if 'final_results_df' in locals() and \"Random Forest (Tuned)\" in final_results_df.index:\n",
    "        print(\"\\n--- Model Evaluation Metrics (With Profit) ---\")\n",
    "        fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "        final_results_df.loc[[\"Random Forest (Tuned)\"]][['Train R2', 'Test R2']].plot(kind='bar', ax=ax1, position=0, width=0.4, color='skyblue')\n",
    "        ax2 = final_results_df.loc[[\"Random Forest (Tuned)\"]][['Train RMSE', 'Test RMSE']].plot(kind='bar', ax=ax1, secondary_y=True, position=1, width=0.4, color='lightcoral')\n",
    "\n",
    "        ax1.set_title('Random Forest (With Profit): Model Evaluation Metrics (R2 and RMSE)')\n",
    "        ax1.set_xlabel('Model')\n",
    "        ax1.set_ylabel('R2 Score')\n",
    "        ax2.set_ylabel('RMSE')\n",
    "\n",
    "        lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "        ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
    "\n",
    "        plt.xticks(rotation=0)\n",
    "        ax1.grid(axis='y')\n",
    "        plt.tight_layout()\n",
    "        print(\"Displaying rf_model_evaluation_metrics plot (With Profit).\")\n",
    "        plt.show()\n",
    "        print(\"\\n--- Inference: Model Evaluation Metrics ---\")\n",
    "        print(\"Comparing train and test scores helps identify overfitting.\")\n",
    "    else:\n",
    "        print(\"\\n'final_results_df' from 'With Profit' analysis not found or does not contain 'Random Forest (Tuned)'. Skipping Model Evaluation Metrics Visualization.\")\n",
    "\n",
    "    if 'feature_importance_df' in locals():\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        top_n = 30\n",
    "        feature_importance_df_sorted = feature_importance_df.sort_values(by='Importance', ascending=False).head(top_n)\n",
    "\n",
    "        plt.barh(feature_importance_df_sorted['Feature'], feature_importance_df_sorted['Importance'])\n",
    "        plt.xlabel('Importance')\n",
    "        plt.title(f'Random Forest (With Profit): Top {top_n} Feature Importances')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        print(\"\\nDisplaying rf_feature_importance plot (With Profit).\")\n",
    "        plt.show()\n",
    "\n",
    "        print(\"\\n--- Detailed Inference from Feature Importance ---\")\n",
    "        print(f\"The top {top_n} features show the estimated contribution of each feature to the model's predictions.\")\n",
    "\n",
    "        top_feature_names = feature_importance_df_sorted['Feature'].tolist()\n",
    "        if top_feature_names:\n",
    "            print(f\"\\nThe most important feature is '{top_feature_names[0]}', contributing significantly to the model's ability to predict sales.\")\n",
    "            if len(top_feature_names) > 1:\n",
    "                print(\"\\nOther highly important features include:\")\n",
    "                for i in range(1, min(top_n, 6)):\n",
    "                    feature = top_feature_names[i]\n",
    "                    importance = feature_importance_df_sorted.iloc[i]['Importance']\n",
    "                    print(f\"- '{feature}' (Importance: {importance:.4f})\")\n",
    "\n",
    "            print(\"\\nThese features likely have a strong correlation with sales and are crucial for the model's performance.\")\n",
    "        else:\n",
    "            print(\"Could not retrieve top feature names for detailed inference.\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\n'feature_importance_df' from 'With Profit' analysis not found. Skipping Feature Importance plot.\")\n",
    "\n",
    "    print(\"\\nRandom Forest Visualization complete (With Profit).\")\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"\\nError: {e}. Please ensure the 'With Profit' model training cells were run successfully before running this visualization cell.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn unexpected error occurred during Random Forest visualization (With Profit): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2hZCzURp7rk"
   },
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JFrCCZ4pG-4Z"
   },
   "outputs": [],
   "source": [
    "# Consolidating visualizations for XGBoost Model (With Profit)\n",
    "\n",
    "print(\"\\n--- Visualizations for XGBoost Model (With Profit) ---\")\n",
    "\n",
    "# Assuming best_xgb_model (with profit) and X_test_selected (with profit) are available\n",
    "# from the initial analysis section.\n",
    "# If you re-ran the \"without profit\" section, these variables might be overwritten.\n",
    "# Ensure you run the \"with profit\" training section before running this cell\n",
    "# if you want the visualizations for the model *with* profit features.\n",
    "\n",
    "try:\n",
    "    xgb_model = best_xgb_model # Use the best XGBoost model from the 'with profit' tuning\n",
    "\n",
    "    # Get predictions and inverse transform for test set\n",
    "    y_test_pred_log_xgb = xgb_model.predict(X_test_selected)\n",
    "    y_test_true_xgb = np.expm1(y_test) # Use y_test from the initial split\n",
    "    y_test_pred_xgb = np.expm1(y_test_pred_log_xgb)\n",
    "\n",
    "    # Calculate residuals\n",
    "    residuals_xgb = y_test_true_xgb - y_test_pred_xgb\n",
    "\n",
    "    # Create a temporary dataframe for plotting, including the original selected features\n",
    "    # Ensure X_test_selected here corresponds to the 'with profit' feature engineering\n",
    "    plot_data_xgb = X_test_selected.copy()\n",
    "    plot_data_xgb['Actual_Sales'] = y_test_true_xgb\n",
    "    plot_data_xgb['Predicted_Sales'] = y_test_pred_xgb\n",
    "    plot_data_xgb['Residuals'] = residuals_xgb\n",
    "\n",
    "    # --- Consolidated Plotting Logic ---\n",
    "\n",
    "    # Residuals vs. Popularity Plot\n",
    "    if 'Product_Popularity' in plot_data_xgb.columns:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(plot_data_xgb['Product_Popularity'], plot_data_xgb['Residuals'], alpha=0.5)\n",
    "        plt.axhline(y=0, color='r', linestyle='--')\n",
    "        plt.title('XGBoost (With Profit): Residuals vs. Product Popularity (Test Set)')\n",
    "        plt.xlabel('Product Popularity (Total Quantity)')\n",
    "        plt.ylabel('Residuals (Actual Sales - Predicted Sales)')\n",
    "        plt.grid(True)\n",
    "        print(\"\\nDisplaying xgb_residuals_vs_popularity plot (With Profit).\")\n",
    "        plt.show()\n",
    "        print(\"\\n--- Inference: Residuals vs. Product Popularity ---\")\n",
    "        print(\"This plot shows if there is a pattern in the errors (residuals) of the model's predictions based on the popularity of a product.\")\n",
    "        print(\"Ideally, the points should be randomly scattered around the zero line.\")\n",
    "\n",
    "    # Predicted vs. Actual by Category Plot\n",
    "    if 'Category' in plot_data_xgb.columns:\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        category_agg_xgb = plot_data_xgb.groupby('Category')[['Actual_Sales', 'Predicted_Sales']].mean().reset_index()\n",
    "        category_agg_xgb.plot(x='Category', y=['Actual_Sales', 'Predicted_Sales'], kind='bar', ax=plt.gca())\n",
    "        plt.title('XGBoost (With Profit): Average Actual vs. Predicted Sales by Category (Test Set)')\n",
    "        plt.xlabel('Category')\n",
    "        plt.ylabel('Average Sales')\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.grid(axis='y')\n",
    "        plt.tight_layout()\n",
    "        print(\"\\nDisplaying xgb_predicted_actual_by_category plot (With Profit).\")\n",
    "        plt.show()\n",
    "        print(\"\\n--- Inference: Predicted vs. Actual by Category ---\")\n",
    "        print(\"This bar plot compares the average actual sales to the average predicted sales for each product category.\")\n",
    "        print(\"Ideally, the 'Predicted_Sales' bar should be close to the 'Actual_Sales' bar.\")\n",
    "\n",
    "\n",
    "    # Distribution of Predicted Sales Values Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(y_test_pred_xgb, bins=50, edgecolor='black')\n",
    "    plt.title('XGBoost (With Profit): Distribution of Predicted Sales Values (Test Set)')\n",
    "    plt.xlabel('Predicted Sales')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    print(\"\\nDisplaying xgb_predicted_sales_distribution plot (With Profit).\")\n",
    "    plt.show()\n",
    "    print(\"\\n--- Inference: Distribution of Predicted Sales Values ---\")\n",
    "    print(\"This histogram shows the frequency distribution of the model's predicted sales values.\")\n",
    "\n",
    "\n",
    "    # Residuals vs Predicted Values Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test_pred_xgb, residuals_xgb, alpha=0.5)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.title('XGBoost (With Profit): Residuals vs. Predicted Values (Test Set)')\n",
    "    plt.xlabel('Predicted Sales')\n",
    "    plt.ylabel('Residuals (Actual Sales - Predicted Sales)')\n",
    "    plt.grid(True)\n",
    "    print(\"\\nDisplaying xgb_residuals_vs_predicted plot (With Profit).\")\n",
    "    plt.show()\n",
    "    print(\"\\n--- Inference: Residuals vs. Predicted Values ---\")\n",
    "    print(\"Randomly scattered points around the zero line are desired. Patterns here (like a funnel shape) suggest heteroscedasticity.\")\n",
    "\n",
    "\n",
    "    # Distribution of Residuals Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(residuals_xgb, bins=50, edgecolor='black')\n",
    "    plt.axvline(x=0, color='r', linestyle='--')\n",
    "    plt.title('XGBoost (With Profit): Distribution of Residuals (Test Set)')\n",
    "    plt.xlabel('Residuals (Actual Sales - Predicted Sales)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    print(\"\\nDisplaying xgb_residuals_distribution plot (With Profit).\")\n",
    "    plt.show()\n",
    "    print(\"\\n--- Inference: Distribution of Residuals ---\")\n",
    "    print(\"For a good model, the residuals should ideally be normally distributed around zero.\")\n",
    "\n",
    "\n",
    "    # Predicted vs. Actual Sales Plot (Scatter Plot)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test_true_xgb, y_test_pred_xgb, alpha=0.5)\n",
    "    plt.plot([y_test_true_xgb.min(), y_test_true_xgb.max()], [y_test_true_xgb.min(), y_test_true_xgb.max()], 'r--', lw=2) # Perfect prediction line\n",
    "    plt.title('XGBoost (With Profit): Predicted vs. Actual Sales (Test Set)')\n",
    "    plt.xlabel('Actual Sales')\n",
    "    plt.ylabel('Predicted Sales')\n",
    "    plt.grid(True)\n",
    "    print(\"\\nDisplaying xgb_predicted_actual_scatter plot (With Profit).\")\n",
    "    plt.show()\n",
    "    print(\"\\n--- Inference: Predicted vs. Actual Sales Scatter Plot ---\")\n",
    "    print(\"Points should ideally lie close to the red dashed line.\")\n",
    "\n",
    "\n",
    "    # Model Evaluation Metrics Visualization (Using the final_results_df from the 'With Profit' run)\n",
    "    # Ensure final_results_df from the 'with profit' run is available\n",
    "    if 'final_results_df' in locals() and \"XGBoost (Tuned)\" in final_results_df.index:\n",
    "        print(\"\\n--- Model Evaluation Metrics (With Profit) ---\")\n",
    "        fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "        final_results_df.loc[[\"XGBoost (Tuned)\"]][['Train R2', 'Test R2']].plot(kind='bar', ax=ax1, position=0, width=0.4, color='skyblue')\n",
    "        ax2 = final_results_df.loc[[\"XGBoost (Tuned)\"]][['Train RMSE', 'Test RMSE']].plot(kind='bar', ax=ax1, secondary_y=True, position=1, width=0.4, color='lightcoral')\n",
    "\n",
    "        ax1.set_title('XGBoost (With Profit): Model Evaluation Metrics (R2 and RMSE)')\n",
    "        ax1.set_xlabel('Model')\n",
    "        ax1.set_ylabel('R2 Score')\n",
    "        ax2.set_ylabel('RMSE')\n",
    "\n",
    "        lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "        ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
    "\n",
    "        plt.xticks(rotation=0)\n",
    "        ax1.grid(axis='y')\n",
    "        plt.tight_layout()\n",
    "        print(\"Displaying xgb_model_evaluation_metrics plot (With Profit).\")\n",
    "        plt.show()\n",
    "        print(\"\\n--- Inference: Model Evaluation Metrics ---\")\n",
    "        print(\"Comparing train and test scores helps identify overfitting.\")\n",
    "    else:\n",
    "        print(\"\\n'final_results_df' from 'With Profit' analysis not found or does not contain 'XGBoost (Tuned)'. Skipping Model Evaluation Metrics Visualization.\")\n",
    "\n",
    "\n",
    "    # Feature importance plot with labels and brief inference\n",
    "    # Ensure feature_importance_df from the 'With Profit' run is available\n",
    "    if 'feature_importance_df' in locals():\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        top_n = 30\n",
    "        # Assuming feature_importance_df contains importances from the 'with profit' run\n",
    "        feature_importance_df_sorted = feature_importance_df.sort_values(by='Importance', ascending=False).head(top_n)\n",
    "\n",
    "        plt.barh(feature_importance_df_sorted['Feature'], feature_importance_df_sorted['Importance'])\n",
    "        plt.xlabel('Importance')\n",
    "        plt.title(f'XGBoost (With Profit): Top {top_n} Feature Importances')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        print(\"\\nDisplaying xgb_feature_importance plot (With Profit).\")\n",
    "        plt.show()\n",
    "\n",
    "        print(\"\\n--- Detailed Inference from Feature Importance ---\")\n",
    "        print(f\"The top {top_n} features show the estimated contribution of each feature to the model's predictions.\")\n",
    "\n",
    "        # Analyze the top features and provide inferences\n",
    "        top_feature_names = feature_importance_df_sorted['Feature'].tolist()\n",
    "        if top_feature_names:\n",
    "            print(f\"\\nThe most important feature is '{top_feature_names[0]}', contributing significantly to the model's ability to predict sales.\")\n",
    "            if len(top_feature_names) > 1:\n",
    "                print(\"\\nOther highly important features include:\")\n",
    "                for i in range(1, min(top_n, 6)):\n",
    "                    feature = top_feature_names[i]\n",
    "                    importance = feature_importance_df_sorted.iloc[i]['Importance']\n",
    "                    print(f\"- '{feature}' (Importance: {importance:.4f})\")\n",
    "\n",
    "            print(\"\\nThese features likely have a strong correlation with sales and are crucial for the model's performance.\")\n",
    "        else:\n",
    "            print(\"Could not retrieve top feature names for detailed inference.\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\n'feature_importance_df' from 'With Profit' analysis not found. Skipping Feature Importance plot.\")\n",
    "\n",
    "    print(\"\\nXGBoost Visualization complete (With Profit).\")\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"\\nError: {e}. Please ensure the 'With Profit' model training cells were run successfully before running this visualization cell.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn unexpected error occurred during XGBoost visualization (With Profit): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5K6efgxUp-Qn"
   },
   "source": [
    "## Without Profit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bE3XkFWrqB1b"
   },
   "source": [
    "### Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xUV_nGyUDku7"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.model_selection import learning_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"\\n--- Visualizations for Linear Regression Model (Without Profit) ---\")\n",
    "\n",
    "try:\n",
    "    lr_model = best_lr_model\n",
    "\n",
    "    y_train_pred_log = lr_model.predict(X_train_selected)\n",
    "    y_train_true = np.expm1(y_train)\n",
    "    y_train_pred = np.expm1(y_train_pred_log)\n",
    "\n",
    "    y_test_pred_log = lr_model.predict(X_test_selected)\n",
    "    y_test_true = np.expm1(y_test)\n",
    "    y_test_pred = np.expm1(y_test_pred_log)\n",
    "\n",
    "    # --- Visualize Feature Importance (for Linear Models, use coefficients) ---\n",
    "    if 'preprocessor_selected' in locals():\n",
    "        fitted_preprocessor = preprocessor_selected.fit(X_train_selected)\n",
    "        feature_names_processed = fitted_preprocessor.get_feature_names_out()\n",
    "        coefficients = lr_model.named_steps['model'].coef_\n",
    "\n",
    "        if len(feature_names_processed) == len(coefficients):\n",
    "             coefficients_df = pd.DataFrame({\n",
    "                 'Feature': feature_names_processed,\n",
    "                 'Coefficient': coefficients\n",
    "             })\n",
    "             coefficients_df = coefficients_df.sort_values(by='Coefficient', ascending=False)\n",
    "\n",
    "             print(\"\\n--- Linear Regression Coefficients (Interpretation depends on preprocessing scale) ---\")\n",
    "             display(coefficients_df)\n",
    "\n",
    "             plt.figure(figsize=(12, min(20, len(coefficients_df) * 0.3)))\n",
    "             top_n_coef = 30\n",
    "             if len(coefficients_df) > 2 * top_n_coef:\n",
    "                 coefs_to_plot = pd.concat([coefficients_df.head(top_n_coef), coefficients_df.tail(top_n_coef)])\n",
    "             else:\n",
    "                 coefs_to_plot = coefficients_df\n",
    "\n",
    "             plt.barh(coefs_to_plot['Feature'], coefs_to_plot['Coefficient'])\n",
    "             plt.xlabel('Coefficient Value')\n",
    "             plt.title('Linear Regression Coefficients (Without Profit)')\n",
    "             plt.gca().invert_yaxis()\n",
    "             plt.tight_layout()\n",
    "             print(\"\\nDisplaying lr_coefficients plot (Without Profit).\")\n",
    "             plt.show()\n",
    "\n",
    "             print(\"\\n--- Inference: Linear Regression Coefficients ---\")\n",
    "             print(\"The magnitude and sign of the coefficients indicate the estimated change in the predicted log-sales for a one-unit increase in the feature value, assuming all other features are held constant.\")\n",
    "             print(\"Positive coefficients suggest a positive relationship with sales, while negative coefficients suggest a negative relationship.\")\n",
    "             print(\"Interpretation is most direct when features are scaled (like with PowerTransformer here), but multicollinearity can still affect individual coefficient values.\")\n",
    "\n",
    "        else:\n",
    "            print(\"\\nWarning: Mismatch between number of features and coefficients. Cannot display coefficients.\")\n",
    "            print(f\"Number of processed features: {len(feature_names_processed)}\")\n",
    "            print(f\"Number of coefficients: {len(coefficients)}\")\n",
    "    else:\n",
    "        print(\"\\n'preprocessor_selected' not found. Cannot display Linear Regression coefficients.\")\n",
    "\n",
    "\n",
    "    # --- Visualize Predicted vs Actual Sales ---\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    axes[0].scatter(y_train_true, y_train_pred, alpha=0.5)\n",
    "    axes[0].plot([y_train_true.min(), y_train_true.max()], [y_train_true.min(), y_train_true.max()], 'k--', lw=2)\n",
    "    axes[0].set_xlabel('Actual Sales (Train)')\n",
    "    axes[0].set_ylabel('Predicted Sales (Train)')\n",
    "    axes[0].set_title('Linear Regression (Without Profit): Actual vs Predicted Sales (Training Set)')\n",
    "\n",
    "    axes[1].scatter(y_test_true, y_test_pred, alpha=0.5)\n",
    "    axes[1].plot([y_test_true.min(), y_test_true.max()], [y_test_true.min(), y_test_true.max()], 'k--', lw=2)\n",
    "    axes[1].set_xlabel('Actual Sales (Test)')\n",
    "    axes[1].set_ylabel('Predicted Sales (Test)')\n",
    "    axes[1].set_title('Linear Regression (Without Profit): Actual vs Predicted Sales (Test Set)')\n",
    "    plt.tight_layout()\n",
    "    print(\"\\nDisplaying lr_predicted_actual_scatter plots (Without Profit).\")\n",
    "    plt.show()\n",
    "    print(\"\\n--- Inference: Predicted vs Actual Sales ---\")\n",
    "    print(\"Points should ideally lie close to the black diagonal line. Scatter indicates prediction errors.\")\n",
    "\n",
    "\n",
    "    # --- Visualize Residuals Distribution ---\n",
    "    train_residuals = y_train_true - y_train_pred\n",
    "    test_residuals = y_test_true - y_test_pred\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    axes[0].hist(train_residuals, bins=50)\n",
    "    axes[0].set_title('Linear Regression (Without Profit): Distribution of Training Set Residuals')\n",
    "    axes[0].set_xlabel('Residuals')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    axes[1].hist(test_residuals, bins=50)\n",
    "    axes[1].set_title('Linear Regression (Without Profit): Distribution of Test Set Residuals')\n",
    "    axes[1].set_xlabel('Residuals')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    print(\"\\nDisplaying lr_residuals_distribution plots (Without Profit).\")\n",
    "    plt.show()\n",
    "    print(\"\\n--- Inference: Residuals Distribution ---\")\n",
    "    print(\"Ideally, residuals should be normally distributed around zero. Skewness or patterns suggest model limitations.\")\n",
    "\n",
    "    # --- Visualize Residuals vs Predicted Values ---\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    axes[0].scatter(y_train_pred, train_residuals, alpha=0.5)\n",
    "    axes[0].axhline(0, color='k', linestyle='--')\n",
    "    axes[0].set_title('Linear Regression (Without Profit): Residuals vs Predicted (Training Set)')\n",
    "    axes[0].set_xlabel('Predicted Sales (Train)')\n",
    "    axes[0].set_ylabel('Residuals (Train)')\n",
    "    axes[0].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    axes[1].scatter(y_test_pred, test_residuals, alpha=0.5)\n",
    "    axes[1].axhline(0, color='k', linestyle='--')\n",
    "    axes[1].set_title('Linear Regression (Without Profit): Residuals vs Predicted (Test Set)')\n",
    "    axes[1].set_xlabel('Predicted Sales (Test)')\n",
    "    axes[1].set_ylabel('Residuals (Test)')\n",
    "    axes[1].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    print(\"\\nDisplaying lr_residuals_vs_predicted plots (Without Profit).\")\n",
    "    plt.show()\n",
    "    print(\"\\n--- Inference: Residuals vs Predicted Values ---\")\n",
    "    print(\"Random scatter around zero is desired. Patterns (like a cone shape) indicate heteroscedasticity.\")\n",
    "\n",
    "\n",
    "    # --- Visualize Predicted vs Actual by Category ---\n",
    "    if 'Category' in X_test_selected.columns:\n",
    "        X_test_selected_plot = X_test_selected.copy()\n",
    "        X_test_selected_plot['Predicted Sales'] = y_test_pred\n",
    "        X_test_selected_plot['Actual Sales'] = y_test_true\n",
    "\n",
    "        sales_by_category = X_test_selected_plot.melt(\n",
    "            id_vars=['Category'],\n",
    "            value_vars=['Actual Sales', 'Predicted Sales'],\n",
    "            var_name='variable',\n",
    "            value_name='value'\n",
    "        )\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.boxplot(data=sales_by_category, x='Category', y='value', hue='variable')\n",
    "        plt.title('Linear Regression (Without Profit): Actual vs Predicted Sales by Category (Test Set)')\n",
    "        plt.xlabel('Category')\n",
    "        plt.ylabel('Sales Value')\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "        print(\"\\nDisplaying lr_predicted_actual_by_category plot (Without Profit).\")\n",
    "        plt.show()\n",
    "        print(\"\\n--- Inference: Predicted vs Actual by Category ---\")\n",
    "        print(\"Compares actual vs predicted sales distributions for each category. Helps identify categories where the model performs better or worse.\")\n",
    "    else:\n",
    "        print(\"\\n'Category' feature not found in selected features. Skipping Predicted vs Actual by Category plot.\")\n",
    "\n",
    "\n",
    "    # --- Visualize Learning Curves ---\n",
    "    print(\"\\n--- Learning Curve Plot for Linear Regression (Without Profit) ---\")\n",
    "    estimator = best_lr_model\n",
    "\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator,\n",
    "        X_train_selected,\n",
    "        y_train,\n",
    "        cv=5,\n",
    "        scoring='r2',\n",
    "        n_jobs=-1,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "        random_state=50\n",
    "    )\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    plt.title('Linear Regression (Without Profit): Learning Curve')\n",
    "    plt.xlabel(\"Training Examples\")\n",
    "    plt.ylabel(\"R2 Score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    print(\"\\nDisplaying lr_learning_curve plot (Without Profit).\")\n",
    "    plt.show()\n",
    "    print(\"\\n--- Inference: Learning Curve ---\")\n",
    "    print(\"Shows how model performance changes with increasing training data. Helps diagnose bias vs variance issues.\")\n",
    "\n",
    "    # --- Additional EDA Plots (Keeping relevant ones) ---\n",
    "    print(\"\\n--- Distribution of Key Features (Original Data) ---\")\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "    sns.histplot(data['Sales'], bins=50, ax=axes[0], kde=True)\n",
    "    axes[0].set_title('Distribution of Original Sales')\n",
    "    axes[0].set_xlabel('Sales Value')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "\n",
    "    sns.histplot(data['Quantity'], bins=20, ax=axes[1], kde=True)\n",
    "    axes[1].set_title('Distribution of Quantity')\n",
    "    axes[1].set_xlabel('Quantity')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "\n",
    "    sns.histplot(data['Discount'], bins=20, ax=axes[2], kde=True)\n",
    "    axes[2].set_title('Distribution of Discount')\n",
    "    axes[2].set_xlabel('Discount')\n",
    "    axes[2].set_ylabel('Frequency')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    print(\"\\nDisplaying distribution plots of original features.\")\n",
    "    plt.show()\n",
    "    print(\"\\n--- Inference: Distribution of Key Features ---\")\n",
    "    print(\"Visualizing original feature distributions helps understand the data and justify preprocessing steps.\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- Relationships Between Key Features (Original Data) ---\")\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    sns.scatterplot(x='Quantity', y='Sales', data=data, alpha=0.5, ax=axes[0])\n",
    "    axes[0].set_title('Sales vs Quantity')\n",
    "    axes[0].set_xlabel('Quantity')\n",
    "    axes[0].set_ylabel('Sales Value')\n",
    "\n",
    "    sns.scatterplot(x='Discount', y='Sales', data=data, alpha=0.5, ax=axes[1])\n",
    "    axes[1].set_title('Sales vs Discount')\n",
    "    axes[1].set_xlabel('Discount')\n",
    "    axes[1].set_ylabel('Sales Value')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    print(\"\\nDisplaying scatter plots of original features vs Sales.\")\n",
    "    plt.show()\n",
    "    print(\"\\n--- Inference: Relationships Between Key Features ---\")\n",
    "    print(\"Shows potential correlations and patterns between input features and the target variable.\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- Sales Trends Over Time (Monthly) ---\")\n",
    "\n",
    "    data['Order Date'] = pd.to_datetime(data['Order Date'], format='%m/%d/%Y', errors='coerce')\n",
    "    monthly_sales = data.set_index('Order Date')['Sales'].resample('M').sum()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    monthly_sales.plot()\n",
    "    plt.title('Monthly Total Sales Over Time')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Total Sales')\n",
    "    plt.grid(True)\n",
    "    print(\"\\nDisplaying monthly sales trend plot.\")\n",
    "    plt.show()\n",
    "    print(\"\\n--- Inference: Sales Trends Over Time ---\")\n",
    "    print(\"Identifies seasonality or trends in sales data.\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- Sales by Other Categorical Features (Original Data) ---\")\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "    sns.boxplot(x='Segment', y='Sales', data=data, ax=axes[0])\n",
    "    axes[0].set_title('Sales by Segment')\n",
    "    axes[0].set_xlabel('Segment')\n",
    "    axes[0].set_ylabel('Sales Value')\n",
    "\n",
    "    sns.boxplot(x='Ship Mode', y='Sales', data=data, ax=axes[1])\n",
    "    axes[1].set_title('Sales by Ship Mode')\n",
    "    axes[1].set_xlabel('Ship Mode')\n",
    "    axes[1].set_ylabel('Sales Value')\n",
    "\n",
    "    sns.boxplot(x='Region', y='Sales', data=data, ax=axes[2])\n",
    "    axes[2].set_title('Sales by Region')\n",
    "    axes[2].set_xlabel('Region')\n",
    "    axes[2].set_ylabel('Sales Value')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"\\n--- Inference: Sales by Other Categorical Features ---\")\n",
    "    print(\"Reveals how sales distributions vary across different categories.\")\n",
    "\n",
    "    print(\"\\nLinear Regression Visualization complete (Without Profit).\")\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"\\nError: {e}. Please ensure the 'Without Profit' model training cells were run successfully before running this visualization cell.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn unexpected error occurred during Linear Regression visualization (Without Profit): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YODqvHaHDmjv"
   },
   "source": [
    "This bar chart displays the relative importance of the top features used by the RandomForestRegressor model to predict sales. Features with taller bars have a greater impact on the model's predictions. We observe that 'Category_Office Supplies' and 'Quantity' are the most dominant features, suggesting that the category of a product and the number of items purchased are the strongest drivers of sales values in this dataset. Features like 'Sub-Category_Furnishings' and 'Sub-Category_Storage' also show significant importance, indicating that within categories, certain sub-categories are more influential. The lower importance of some other features suggests they contribute less to the predictive power of this particular model. Inference: Focusing on understanding the dynamics of 'Office Supplies' category and strategies related to 'Quantity' and high-importance sub-categories could be key to improving sales predictions and potentially sales performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LLRRaQjiqVgs"
   },
   "outputs": [],
   "source": [
    "# --- Visualize Predicted vs Actual Sales ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Training set plot\n",
    "axes[0].scatter(y_train_true, y_train_pred, alpha=0.5)\n",
    "axes[0].plot([y_train_true.min(), y_train_true.max()], [y_train_true.min(), y_train_true.max()], 'k--', lw=2)\n",
    "axes[0].set_xlabel('Actual Sales (Train)')\n",
    "axes[0].set_ylabel('Predicted Sales (Train)')\n",
    "axes[0].set_title('Actual vs Predicted Sales (Training Set)')\n",
    "axes[0].set_aspect('equal', adjustable='box')\n",
    "\n",
    "# Test set plot\n",
    "axes[1].scatter(y_test_true, y_test_pred, alpha=0.5)\n",
    "axes[1].plot([y_test_true.min(), y_test_true.max()], [y_test_true.min(), y_test_true.max()], 'k--', lw=2)\n",
    "axes[1].set_xlabel('Actual Sales (Test)')\n",
    "axes[1].set_ylabel('Predicted Sales (Test)')\n",
    "axes[1].set_title('Actual vs Predicted Sales (Test Set)')\n",
    "axes[1].set_aspect('equal', adjustable='box')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZ20F7pZD98O"
   },
   "source": [
    "These scatter plots compare the model's predicted sales values against the actual sales values for both the training and test datasets. A perfect model would have all points lying exactly on the black diagonal line, where predicted equals actual. We observe that while there is a general trend along the line, there is considerable scatter, particularly for higher sales values. This indicates that the model has difficulty accurately predicting larger sales amounts. The scatter appears somewhat similar between the training and test sets, which is good as it doesn't strongly suggest overfitting on the training data based on this plot alone. Inference: The model struggles to capture the variability in higher sales transactions. Further investigation into the characteristics of high-value sales and potentially using a model better suited for capturing extreme values might be beneficial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AYvInw25D4EO"
   },
   "outputs": [],
   "source": [
    "# --- Visualize Residuals Distribution ---\n",
    "train_residuals = y_train_true - y_train_pred\n",
    "test_residuals = y_test_true - y_test_pred\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "axes[0].hist(train_residuals, bins=50)\n",
    "axes[0].set_title('Distribution of Training Set Residuals')\n",
    "axes[0].set_xlabel('Residuals')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "axes[1].hist(test_residuals, bins=50)\n",
    "axes[1].set_title('Distribution of Test Set Residuals')\n",
    "axes[1].set_xlabel('Residuals')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "seTBFcBXEEmC"
   },
   "source": [
    "These histograms show the distribution of the residuals, which are the differences between the actual and predicted sales values (Actual - Predicted). For a good regression model, we ideally expect the residuals to be normally distributed around zero. Looking at the plots, both the training and test residuals appear somewhat centered around zero, but they exhibit a skewed distribution, particularly with a long tail extending towards positive values. This positive skew suggests that the model tends to under-predict sales more often than it over-predicts, especially for larger errors. Inference: The non-normal and skewed distribution of residuals indicates that the model's assumptions might be violated, and there's room for improvement in capturing the underlying patterns, potentially by addressing outliers or considering different modeling techniques.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9O_LLTzTD4iU"
   },
   "outputs": [],
   "source": [
    "# --- Visualize Residuals vs Predicted Values ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Training set residuals vs predicted\n",
    "axes[0].scatter(y_train_pred, train_residuals, alpha=0.5)\n",
    "axes[0].axhline(0, color='k', linestyle='--')\n",
    "axes[0].set_title('Residuals vs Predicted (Training Set)')\n",
    "axes[0].set_xlabel('Predicted Sales (Train)')\n",
    "axes[0].set_ylabel('Residuals (Train)')\n",
    "\n",
    "# Test set residuals vs predicted\n",
    "axes[1].scatter(y_test_pred, test_residuals, alpha=0.5)\n",
    "axes[1].axhline(0, color='k', linestyle='--')\n",
    "axes[1].set_title('Residuals vs Predicted (Test Set)')\n",
    "axes[1].set_xlabel('Predicted Sales (Test)')\n",
    "axes[1].set_ylabel('Residuals (Test)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RsY0QccqEMOR"
   },
   "source": [
    "These scatter plots show the relationship between the predicted sales values and the corresponding residuals. For a well-performing model with homoscedastic errors (constant variance), we would expect to see a random cloud of points scattered evenly around the horizontal line at zero, with no discernible pattern. In these plots, we observe a pattern where the spread of residuals increases as the predicted sales values increase, forming a cone-like shape. This pattern indicates heteroscedasticity, meaning the model's errors are not consistent across all predicted values; it tends to make larger errors for higher predicted sales. Inference: The presence of heteroscedasticity suggests that the model's assumptions about constant error variance are not met. This can impact the reliability of statistical tests and confidence intervals. Addressing this might involve transforming the target variable further, using robust regression techniques, or modeling the variance separately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EBZCxQwsD5Dq"
   },
   "outputs": [],
   "source": [
    "# --- Visualize Predicted vs Actual by Category ---\n",
    "X_test_selected_plot = X_test_selected.copy()\n",
    "X_test_selected_plot['Predicted Sales'] = y_test_pred\n",
    "X_test_selected_plot['Actual Sales'] = y_test_true\n",
    "\n",
    "sales_by_category = X_test_selected_plot.melt(\n",
    "    id_vars=['Category'],\n",
    "    value_vars=['Actual Sales', 'Predicted Sales'],\n",
    "    var_name='variable',\n",
    "    value_name='value'\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=sales_by_category, x='Category', y='value', hue='variable')\n",
    "plt.title('Actual vs Predicted Sales by Category')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Sales Value')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yuIvNJQXET4l"
   },
   "source": [
    "This box plot visualizes the distribution of actual and predicted sales values for each major product category. By comparing the boxes and whiskers for 'Actual Sales' and 'Predicted Sales' within each category, we can assess how well the model is performing for different product types. We can observe that the distributions for 'Office Supplies' appear relatively similar between actual and predicted, suggesting better performance in this category. For 'Furniture' and 'Technology', there seem to be larger discrepancies, particularly in the spread and potential outliers, indicating that the model may be less accurate in predicting sales for these categories. Inference: The model's performance varies significantly across product categories. Further analysis focusing on 'Furniture' and 'Technology' categories might reveal specific patterns or features that the current model is not effectively capturing, leading to poorer predictions in these areas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2m9TPYerETkk"
   },
   "outputs": [],
   "source": [
    "# --- Visualize Learning Curves ---\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    best_lr_model,\n",
    "    X_train_selected,\n",
    "    y_train,\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    random_state=50\n",
    ")\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "plt.title('Learning Curve for Linear Regression Model')\n",
    "plt.xlabel(\"Training Examples\")\n",
    "plt.ylabel(\"R2 Score\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skvYiWJPEccl"
   },
   "source": [
    "The learning curve illustrates how the model's performance, measured by R2 score, changes as the amount of training data increases. The red line represents the training score, and the green line represents the cross-validation score (performance on unseen data during training). We observe that the training score starts high and decreases slightly, while the cross-validation score starts lower and plateaus as more training examples are added. There is a noticeable gap between the training score and the cross-validation score even with the full training dataset. This pattern typically indicates overfitting; the model is performing much better on the data it was trained on than on new, unseen data. Inference: The model is likely too complex for the amount of data available or is capturing noise in the training data. To address this, we could consider simplifying the model, using regularization techniques, or potentially acquiring more diverse training data if possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DPTV7h0_ETcj"
   },
   "outputs": [],
   "source": [
    "# Distribution of Key Features (Original Sales, Quantity, Discount)\n",
    "print(\"\\n--- Distribution of Key Features ---\")\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "sns.histplot(data['Sales'], bins=50, ax=axes[0], kde=True)\n",
    "axes[0].set_title('Distribution of Original Sales')\n",
    "axes[0].set_xlabel('Sales Value')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "sns.histplot(data['Quantity'], bins=20, ax=axes[1], kde=True)\n",
    "axes[1].set_title('Distribution of Quantity')\n",
    "axes[1].set_xlabel('Quantity')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "sns.histplot(data['Discount'], bins=20, ax=axes[2], kde=True)\n",
    "axes[2].set_title('Distribution of Discount')\n",
    "axes[2].set_xlabel('Discount')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WT_IhKV0Ep23"
   },
   "source": [
    "Explanation & Inference:\n",
    "\n",
    "These histograms show the distributions of the original Sales, Quantity, and Discount features. The Sales distribution is highly skewed to the right, indicating many small sales and a few very large ones. This skewness is why we applied a log transformation to the target variable. Quantity shows a distribution concentrated on smaller values. Discount has a distribution with peaks at 0 and various discount levels, suggesting discrete discount policies. Inference: Understanding these distributions is crucial for preprocessing steps like transformation and outlier handling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SdXPEkDnETU0"
   },
   "outputs": [],
   "source": [
    "# Relationships Between Key Features (Sales vs Quantity, Sales vs Discount)\n",
    "print(\"\\n--- Relationships Between Key Features ---\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "sns.scatterplot(x='Quantity', y='Sales', data=data, alpha=0.5, ax=axes[0])\n",
    "axes[0].set_title('Sales vs Quantity')\n",
    "axes[0].set_xlabel('Quantity')\n",
    "axes[0].set_ylabel('Sales Value')\n",
    "\n",
    "sns.scatterplot(x='Discount', y='Sales', data=data, alpha=0.5, ax=axes[1])\n",
    "axes[1].set_title('Sales vs Discount')\n",
    "axes[1].set_xlabel('Discount')\n",
    "axes[1].set_ylabel('Sales Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-YhLEYPE1UG"
   },
   "source": [
    "Explanation & Inference:\n",
    "\n",
    "These scatter plots show the relationship between Sales and Quantity, and Sales and Discount. Sales generally tend to increase with Quantity, although there's considerable spread. The relationship between Sales and Discount appears more complex, with high sales values occurring across various discount levels, and some high discounts associated with lower sales (possibly due to loss-leading strategies or attempts to move unpopular items). Inference: These plots highlight the non-linear and potentially complex relationships between features and the target, which models like Linear Regression might struggle to capture fully.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1rKSfH_VE5z4"
   },
   "outputs": [],
   "source": [
    "# Sales Trends Over Time (Monthly Sales)\n",
    "print(\"\\n--- Sales Trends Over Time (Monthly) ---\")\n",
    "\n",
    "data['Order Date'] = pd.to_datetime(data['Order Date'], format='%m/%d/%Y')\n",
    "monthly_sales = data.set_index('Order Date')['Sales'].resample('M').sum()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "monthly_sales.plot()\n",
    "plt.title('Monthly Total Sales Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Total Sales')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xu_wSSuE1LK"
   },
   "source": [
    "Explanation & Inference:\n",
    "\n",
    "This line plot shows the total sales aggregated by month over the years in the dataset. We can observe potential trends and seasonality. There appear to be yearly cycles, with sales often peaking towards the end of the year. Inference: Time-based features and potentially time series models could be valuable for capturing these temporal patterns.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qZBeJLZ2D5gb"
   },
   "outputs": [],
   "source": [
    "# Sales by Other Categorical Features (Segment, Ship Mode, Region)\n",
    "print(\"\\n--- Sales by Other Categorical Features ---\")\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "sns.boxplot(x='Segment', y='Sales', data=data, ax=axes[0])\n",
    "axes[0].set_title('Sales by Segment')\n",
    "axes[0].set_xlabel('Segment')\n",
    "axes[0].set_ylabel('Sales Value')\n",
    "\n",
    "sns.boxplot(x='Ship Mode', y='Sales', data=data, ax=axes[1])\n",
    "axes[1].set_title('Sales by Ship Mode')\n",
    "axes[1].set_xlabel('Ship Mode')\n",
    "axes[1].set_ylabel('Sales Value')\n",
    "\n",
    "sns.boxplot(x='Region', y='Sales', data=data, ax=axes[2])\n",
    "axes[2].set_title('Sales by Region')\n",
    "axes[2].set_xlabel('Region')\n",
    "axes[2].set_ylabel('Sales Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJrDGZJVFBnV"
   },
   "source": [
    "Explanation & Inference:\n",
    "\n",
    "These box plots show the distribution of sales values for different categories within Segment, Ship Mode, and Region. They help identify if certain categories within these features are associated with significantly different sales ranges or distributions. For example, one segment might have higher median sales or more outliers than another. Inference: These categorical features appear to have varying relationships with sales and are important to include in the model, as confirmed by feature importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dv2KiNOdqErf"
   },
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x7Oe_EeHqpbT"
   },
   "outputs": [],
   "source": [
    "# Consolidating Residuals vs Product Popularity Plots for Random Forest Model (Without Profit)\n",
    "\n",
    "print(\"\\n--- Residuals vs Product Popularity Plots for Random Forest (Without Profit) ---\")\n",
    "\n",
    "try:\n",
    "    name = \"RandomForest (Tuned)\"\n",
    "    model = best_rf_model\n",
    "\n",
    "    y_train_pred_log = model.predict(X_train_selected)\n",
    "    y_train_pred = np.expm1(y_train_pred_log)\n",
    "    y_train_true = np.expm1(y_train)\n",
    "\n",
    "    train_residuals = y_train_true - y_train_pred\n",
    "\n",
    "    y_test_pred_log = model.predict(X_test_selected)\n",
    "    y_test_pred = np.expm1(y_test_pred_log)\n",
    "    y_test_true = np.expm1(y_test)\n",
    "\n",
    "    test_residuals = y_test_true - y_test_pred\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    if 'Product_Popularity' in X_train_selected.columns:\n",
    "        axes[0].scatter(X_train_selected['Product_Popularity'], train_residuals, alpha=0.5)\n",
    "        axes[0].axhline(0, color='k', linestyle='--', lw=2)\n",
    "        axes[0].set_title(f'{name} (Without Profit): Residuals vs. Product Popularity (Train Set)')\n",
    "        axes[0].set_xlabel('Product Popularity')\n",
    "        axes[0].set_ylabel('Residuals (Actual - Predicted)')\n",
    "        axes[0].grid(True, linestyle='--', alpha=0.6)\n",
    "    else:\n",
    "        axes[0].set_title(f'{name} (Without Profit): Product_Popularity not in selected features (Train Set)')\n",
    "\n",
    "\n",
    "    if 'Product_Popularity' in X_test_selected.columns:\n",
    "        axes[1].scatter(X_test_selected['Product_Popularity'], test_residuals, alpha=0.5, color='salmon')\n",
    "        axes[1].axhline(0, color='k', linestyle='--', lw=2)\n",
    "        axes[1].set_title(f'{name} (Without Profit): Residuals vs. Product Popularity (Test Set)')\n",
    "        axes[1].set_xlabel('Product Popularity')\n",
    "        axes[1].set_ylabel('Residuals (Actual - Predicted)')\n",
    "        axes[1].grid(True, linestyle='--', alpha=0.6)\n",
    "    else:\n",
    "         axes[1].set_title(f'{name} (Without Profit): Product_Popularity not in selected features (Test Set)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    print(\"\\nDisplaying rf_residuals_vs_popularity plots (Without Profit).\")\n",
    "    plt.show()\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"\\nError: {e}. Please ensure the 'Without Profit' model training cells were run successfully before running this visualization cell.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn unexpected error occurred during Random Forest visualization (Without Profit): {e}\")\n",
    "\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "try:\n",
    "    name = \"RandomForest (Tuned)\"\n",
    "    model = best_rf_model\n",
    "\n",
    "    print(f\"\\n--- Learning Curve Plot for {name} (Without Profit) ---\")\n",
    "\n",
    "    estimator = model\n",
    "\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator,\n",
    "        X_train_selected,\n",
    "        y_train,\n",
    "        cv=5,\n",
    "        scoring='r2',\n",
    "        n_jobs=-1,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "        random_state=50\n",
    "    )\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "\n",
    "    plt.title(f'{name} (Without Profit): Learning Curve')\n",
    "    plt.xlabel(\"Training Examples\")\n",
    "    plt.ylabel(\"R2 Score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    print(\"\\nDisplaying rf_learning_curve plot (Without Profit).\")\n",
    "    plt.show()\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"\\nError: {e}. Please ensure the 'Without Profit' model training cells were run successfully before running this visualization cell.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn unexpected error occurred during Random Forest Learning Curve plot (Without Profit): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKS2ILX1rGIu"
   },
   "source": [
    "Inference:\n",
    "\n",
    "RandomForest (Tuned):\n",
    "\n",
    "* The learning curve for the\n",
    "RandomForest model shows a smaller gap between the training score and the cross-validation score compared to the XGBoost model. This suggests that the RandomForest model is overfitting less than the XGBoost model, although there is still some overfitting present.\n",
    "* Both the training and cross-validation scores appear to be leveling off as the training set size increases, indicating that adding more data might not significantly improve the performance of the current RandomForest model configuration.\n",
    "* The overall R2 scores on both the training and cross-validation sets are lower for the RandomForest model compared to the XGBoost model, which aligns with the final evaluation metrics and suggests that, with the current tuning, XGBoost performs better on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qBzdYOLFrfFg"
   },
   "outputs": [],
   "source": [
    "# Consolidating Predicted vs. Actual by Category Plots for Random Forest Model (Without Profit)\n",
    "\n",
    "print(f\"\\n--- Predicted vs. Actual by Category Plots for Random Forest (Without Profit) ---\")\n",
    "\n",
    "try:\n",
    "    name = \"RandomForest (Tuned)\"\n",
    "    model = best_rf_model\n",
    "\n",
    "    y_train_pred_log = model.predict(X_train_selected)\n",
    "    y_train_pred = np.expm1(y_train_pred_log)\n",
    "    y_train_true = np.expm1(y_train)\n",
    "\n",
    "    y_test_pred_log = model.predict(X_test_selected)\n",
    "    y_test_pred = np.expm1(y_test_pred_log)\n",
    "    y_test_true = np.expm1(y_test)\n",
    "\n",
    "    X_train_selected_plot = X_train_selected.copy()\n",
    "    X_train_selected_plot['Actual Sales'] = y_train_true\n",
    "    X_train_selected_plot['Predicted Sales'] = y_train_pred\n",
    "\n",
    "    X_test_selected_plot = X_test_selected.copy()\n",
    "    X_test_selected_plot['Actual Sales'] = y_test_true\n",
    "    X_test_selected_plot['Predicted Sales'] = y_test_pred\n",
    "\n",
    "\n",
    "    if 'Category' in X_train_selected_plot.columns:\n",
    "        categories = X_train_selected_plot['Category'].unique()\n",
    "        n_categories = len(categories)\n",
    "\n",
    "        n_cols = 2\n",
    "        n_rows = (n_categories + n_cols - 1) // n_cols\n",
    "\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, n_rows * 5))\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        for i, category in enumerate(categories):\n",
    "            ax = axes[i]\n",
    "\n",
    "            train_cat_data = X_train_selected_plot[X_train_selected_plot['Category'] == category]\n",
    "            test_cat_data = X_test_selected_plot[X_test_selected_plot['Category'] == category]\n",
    "\n",
    "            ax.scatter(train_cat_data['Actual Sales'], train_cat_data['Predicted Sales'], alpha=0.5, label='Train')\n",
    "            ax.scatter(test_cat_data['Actual Sales'], test_cat_data['Predicted Sales'], alpha=0.5, label='Test', color='salmon')\n",
    "\n",
    "            max_sales = max(train_cat_data['Actual Sales'].max(), test_cat_data['Actual Sales'].max())\n",
    "            ax.plot([0, max_sales], [0, max_sales], 'k--', lw=2)\n",
    "\n",
    "            ax.set_title(f'{category}')\n",
    "            ax.set_xlabel('Actual Sales')\n",
    "            ax.set_ylabel('Predicted Sales')\n",
    "            ax.legend()\n",
    "            ax.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "        for j in range(i + 1, len(axes)):\n",
    "            fig.delaxes(axes[j])\n",
    "\n",
    "        plt.suptitle(f'{name} (Without Profit): Predicted vs. Actual Sales by Category', y=1.02, fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        print(\"\\nDisplaying rf_predicted_actual_by_category plots (Without Profit).\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"\\n'Category' feature not found in selected features. Skipping this plot.\")\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"\\nError: {e}. Please ensure the 'Without Profit' model training cells were run successfully before running this visualization cell.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn unexpected error occurred during Random Forest visualization (Without Profit): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0lcUF-Grgr_"
   },
   "source": [
    "Inference:\n",
    "\n",
    "RandomForest (Tuned):\n",
    "\n",
    "* The plots show the performance of the model on predicting sales within each product category.\n",
    "* For categories with a wider range of sales values (e.g., Technology, Furniture), the spread of predicted values is larger, but the model still tends to under-predict the highest sales values, similar to the XGBoost model, but the overall fit appears less tight compared to XGBoost on the training data.\n",
    "* For categories with generally lower sales values (e.g., Office Supplies), the predictions are more clustered, and the model appears to perform reasonably well, although some under-prediction of slightly higher values within these categories is also visible.\n",
    "* The test set points generally follow a similar pattern to the training set within each category, but the spread around the diagonal line is wider, indicating the model is less accurate on unseen data for all categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bzz2IBrI-bon"
   },
   "outputs": [],
   "source": [
    "# Consolidating Distribution of Predicted Sales Values Plots for Random Forest Model (Without Profit)\n",
    "\n",
    "print(f\"\\n--- Distribution of Predicted Sales Values Plots for Random Forest (Without Profit) ---\")\n",
    "\n",
    "try:\n",
    "    name = \"RandomForest (Tuned)\"\n",
    "    model = best_rf_model\n",
    "\n",
    "    y_train_pred_log = model.predict(X_train_selected)\n",
    "    y_train_pred = np.expm1(y_train_pred_log)\n",
    "\n",
    "    y_test_pred_log = model.predict(X_test_selected)\n",
    "    y_test_pred = np.expm1(y_test_pred_log)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    axes[0].hist(y_train_pred, bins=50, edgecolor='k', alpha=0.7)\n",
    "    axes[0].set_title(f'{name} (Without Profit): Distribution of Predicted Sales Values (Train Set)')\n",
    "    axes[0].set_xlabel('Predicted Sales Values')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "\n",
    "    axes[1].hist(y_test_pred, bins=50, edgecolor='k', alpha=0.7, color='salmon')\n",
    "    axes[1].set_title(f'{name} (Without Profit): Distribution of Predicted Sales Values (Test Set)')\n",
    "    axes[1].set_xlabel('Predicted Sales Values')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    print(\"\\nDisplaying rf_predicted_sales_distribution plots (Without Profit).\")\n",
    "    plt.show()\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"\\nError: {e}. Please ensure the 'Without Profit' model training cells were run successfully before running this visualization cell.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn unexpected error occurred during Random Forest visualization (Without Profit): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXJzFjIf-leq"
   },
   "source": [
    "Inference:\n",
    "\n",
    "RandomForest (Tuned):\n",
    "\n",
    "* Training Set: The distribution of predicted sales values for the training set is heavily skewed to the right, with a strong peak at lower predicted values and a long tail. This reflects the skewness in the actual sales data but also suggests the model might be overly cautious in predicting higher values.\n",
    "* Test Set: The distribution of predicted sales values for the test set is similar to the training set, also exhibiting significant right-skewness. The range of predicted values on the test set appears somewhat compressed compared to the training set, further indicating that the model is less confident in predicting high sales values on unseen data. The distributions for both train and test sets do not perfectly match the distribution of actual sales, suggesting the model might not be fully capturing the patterns of high sales values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JSJoyvvi-bj0"
   },
   "outputs": [],
   "source": [
    "# Consolidating Residuals vs Predicted Values Plots for Random Forest Model (Without Profit)\n",
    "\n",
    "print(f\"\\n--- Residuals vs Predicted Values Plots for Random Forest (Without Profit) ---\")\n",
    "\n",
    "try:\n",
    "    name = \"RandomForest (Tuned)\"\n",
    "    model = best_rf_model\n",
    "\n",
    "    y_train_pred_log = model.predict(X_train_selected)\n",
    "    y_train_pred = np.expm1(y_train_pred_log)\n",
    "    y_train_true = np.expm1(y_train)\n",
    "\n",
    "    train_residuals = y_train_true - y_train_pred\n",
    "\n",
    "    y_test_pred_log = model.predict(X_test_selected)\n",
    "    y_test_pred = np.expm1(y_test_pred_log)\n",
    "    y_test_true = np.expm1(y_test)\n",
    "\n",
    "    test_residuals = y_test_true - y_test_pred\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    axes[0].scatter(y_train_pred, train_residuals, alpha=0.5)\n",
    "    axes[0].axhline(0, color='k', linestyle='--', lw=2)\n",
    "    axes[0].set_title(f'{name} (Without Profit): Residuals vs. Predicted Values (Train Set)')\n",
    "    axes[0].set_xlabel('Predicted Sales')\n",
    "    axes[0].set_ylabel('Residuals (Actual - Predicted)')\n",
    "    axes[0].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    axes[1].scatter(y_test_pred, test_residuals, alpha=0.5, color='salmon')\n",
    "    axes[1].axhline(0, color='k', linestyle='--', lw=2)\n",
    "    axes[1].set_title(f'{name} (Without Profit): Residuals vs. Predicted Values (Test Set)')\n",
    "    axes[1].set_xlabel('Predicted Sales')\n",
    "    axes[1].set_ylabel('Residuals (Actual - Predicted)')\n",
    "    axes[1].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    print(\"\\nDisplaying rf_residuals_vs_predicted plots (Without Profit).\")\n",
    "    plt.show()\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"\\nError: {e}. Please ensure the 'Without Profit' model training cells were run successfully before running this visualization cell.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn unexpected error occurred during Random Forest visualization (Without Profit): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7f6fppBy-yUU"
   },
   "source": [
    "Inference:\n",
    "\n",
    "RandomForest (Tuned):\n",
    "\n",
    "* Training Set: The residuals plot for the training set shows a similar pattern to XGBoost, with residuals spreading out as predicted sales values increase, indicating heteroscedasticity. The points are somewhat centered around zero, but the spread is not uniform. There are also some noticeable clusters of residuals, suggesting potential issues with specific groups of data points.\n",
    "* Test Set: The pattern of heteroscedasticity is also present in the test set residuals plot, with a wider spread of errors for higher predicted values compared to the training set. This reinforces the observation that the model struggles more with predicting higher sales values on unseen data. The overall scatter is larger than on the training set, consistent with the lower test R2 and higher test RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q15Ak_nu-be-"
   },
   "outputs": [],
   "source": [
    "# Consolidating Distribution of Residuals Plots for Random Forest Model (Without Profit)\n",
    "\n",
    "print(f\"\\n--- Residuals Plots for Random Forest (Without Profit) ---\")\n",
    "\n",
    "try:\n",
    "    name = \"RandomForest (Tuned)\"\n",
    "    model = best_rf_model\n",
    "\n",
    "    y_train_pred_log = model.predict(X_train_selected)\n",
    "    y_train_pred = np.expm1(y_train_pred_log)\n",
    "    y_train_true = np.expm1(y_train)\n",
    "\n",
    "    train_residuals = y_train_true - y_train_pred\n",
    "\n",
    "    y_test_pred_log = model.predict(X_test_selected)\n",
    "    y_test_pred = np.expm1(y_test_pred_log)\n",
    "    y_test_true = np.expm1(y_test)\n",
    "\n",
    "    test_residuals = y_test_true - y_test_pred\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    axes[0].hist(train_residuals, bins=50, edgecolor='k', alpha=0.7)\n",
    "    axes[0].set_title(f'{name} (Without Profit): Distribution of Training Residuals')\n",
    "    axes[0].set_xlabel('Training Residuals')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "\n",
    "    axes[1].hist(test_residuals, bins=50, edgecolor='k', alpha=0.7, color='salmon')\n",
    "    axes[1].set_title(f'{name} (Without Profit): Distribution of Test Residuals')\n",
    "    axes[1].set_xlabel('Test Residuals')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    print(\"\\nDisplaying rf_residuals_distribution plots (Without Profit).\")\n",
    "    plt.show()\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"\\nError: {e}. Please ensure the 'Without Profit' model training cells were run successfully before running this visualization cell.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn unexpected error occurred during Random Forest visualization (Without Profit): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF92uQ5D-6g7"
   },
   "source": [
    "Inference:\n",
    "\n",
    "RandomForest (Tuned):\n",
    "\n",
    "* Training Set: The distribution of training residuals is skewed to the right, with a peak around zero and a long tail of positive residuals. This suggests that the model tends to under-predict sales values, particularly for higher sales amounts, resulting in large positive residuals.\n",
    "* Test Set: The distribution of test residuals shows a similar right-skewness, but the spread appears wider and the tail of positive residuals is more pronounced compared to the training set. This indicates that the under-prediction of higher sales values is more significant on unseen data, contributing to the lower performance on the test set. The distribution is not centered at zero, suggesting a systematic under-prediction bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vg6OklFr-bZO"
   },
   "outputs": [],
   "source": [
    "# Consolidating Predicted vs. Actual Sales Plots for Random Forest Model (Without Profit)\n",
    "\n",
    "print(f\"\\n--- Predicted vs. Actual Sales Plots for Random Forest (Without Profit) ---\")\n",
    "\n",
    "try:\n",
    "    name = \"RandomForest (Tuned)\"\n",
    "    model = best_rf_model\n",
    "\n",
    "    y_train_pred_log = model.predict(X_train_selected)\n",
    "    y_train_pred = np.expm1(y_train_pred_log)\n",
    "    y_train_true = np.expm1(y_train)\n",
    "\n",
    "    y_test_pred_log = model.predict(X_test_selected)\n",
    "    y_test_pred = np.expm1(y_test_pred_log)\n",
    "    y_test_true = np.expm1(y_test)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    axes[0].scatter(y_train_true, y_train_pred, alpha=0.5)\n",
    "    axes[0].plot([y_train_true.min(), y_train_true.max()], [y_train_true.min(), y_train_true.max()], 'k--', lw=2)\n",
    "    axes[0].set_title(f'{name} (Without Profit): Predicted vs. Actual Sales (Train Set)')\n",
    "    axes[0].set_xlabel('Actual Sales')\n",
    "    axes[0].set_ylabel('Predicted Sales')\n",
    "    axes[0].set_xlim([0, y_train_true.max() * 1.05])\n",
    "    axes[0].set_ylim([0, y_train_true.max() * 1.05])\n",
    "    axes[0].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "\n",
    "    axes[1].scatter(y_test_true, y_test_pred, alpha=0.5, color='salmon')\n",
    "    axes[1].plot([y_test_true.min(), y_test_true.max()], [y_test_true.min(), y_test_true.max()], 'k--', lw=2)\n",
    "    axes[1].set_title(f'{name} (Without Profit): Predicted vs. Actual Sales (Test Set)')\n",
    "    axes[1].set_xlabel('Actual Sales')\n",
    "    axes[1].set_ylabel('Predicted Sales')\n",
    "    axes[1].set_xlim([0, y_test_true.max() * 1.05])\n",
    "    axes[1].set_ylim([0, y_test_true.max() * 1.05])\n",
    "    axes[1].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    print(\"\\nDisplaying rf_predicted_actual_scatter plots (Without Profit).\")\n",
    "    plt.show()\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"\\nError: {e}. Please ensure the 'Without Profit' model training cells were run successfully before running this visualization cell.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn unexpected error occurred during Random Forest visualization (Without Profit): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvvW5Puw_ERc"
   },
   "source": [
    "Inference:\n",
    "\n",
    "RandomForest (Tuned):\n",
    "\n",
    "* Training Set: The scatter plot for the training set shows that the model's predictions generally follow the trend of actual sales but exhibit significant spread, especially at higher sales values. The model tends to under-predict high sales values, similar to the XGBoost model, but the overall fit appears less tight compared to XGBoost on the training data.\n",
    "* Test Set: The plot for the test set shows a similar pattern of under-predicting high sales values and a wider scatter of points around the diagonal line compared to the training set. This indicates poorer performance and generalization on unseen data, consistent with the evaluation metrics. The under-prediction of high values is more pronounced on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rp-w9Nsk-bRq"
   },
   "outputs": [],
   "source": [
    "# Consolidating Model Evaluation Metrics Visualization for Random Forest (Without Profit)\n",
    "\n",
    "print(f\"\\n--- Final Model Plots for Random Forest (Without Profit) ---\")\n",
    "\n",
    "try:\n",
    "    name = \"RandomForest (Tuned)\"\n",
    "\n",
    "    if name in final_results_df.index:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "        final_results_df.loc[[name]][['Train R2', 'Test R2']].plot(kind='bar', ax=axes[0])\n",
    "        axes[0].set_title(f'{name} (Without Profit): R2 Scores (Train vs Test)')\n",
    "        axes[0].set_ylabel('R2 Score')\n",
    "        axes[0].tick_params(axis='x', rotation=0)\n",
    "        axes[0].legend(title='Dataset')\n",
    "        axes[0].grid(axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "\n",
    "        final_results_df.loc[[name]][['Train RMSE', 'Test RMSE']].plot(kind='bar', ax=axes[1], color=['lightcoral', 'salmon'])\n",
    "        axes[1].set_title(f'{name} (Without Profit): RMSE Scores (Train vs Test)')\n",
    "        axes[1].set_ylabel('RMSE')\n",
    "        axes[1].tick_params(axis='x', rotation=0)\n",
    "        axes[1].legend(title='Dataset')\n",
    "        axes[1].grid(axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "\n",
    "        plt.tight_layout()\n",
    "        print(\"\\nDisplaying rf_model_evaluation_metrics plot (Without Profit).\")\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        print(f\"\\nResults for '{name}' not found in 'final_results_df'. Skipping plots.\")\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"\\nError: {e}. Please ensure the 'Without Profit' model training cells were run successfully before running this visualization cell.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn unexpected error occurred during Random Forest evaluation plots (Without Profit): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KM54USr6_Jfb"
   },
   "source": [
    "Inference:\n",
    "\n",
    "RandomForest (Tuned):\n",
    "\n",
    "* The bar plots show the R2 and RMSE scores for the tuned RandomForest model on both the training and test sets.\n",
    "* The training R2 score is significantly higher than the test R2 score, and the training RMSE is much lower than the test RMSE. This indicates a substantial performance drop on unseen data, suggesting that the RandomForest model is also overfitting the training data.\n",
    "* Compared to the XGBoost model, the RandomForest model has lower R2 scores and higher RMSE values on both the training and test sets, suggesting that XGBoost performed better overall on this dataset with the current tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TVPkbeUt-bK2"
   },
   "outputs": [],
   "source": [
    "# Plot feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Feature Importances from Temporary RandomForest Model')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKrVwzkdqGqj"
   },
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dT9_NUvdqAjD"
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- Residuals vs Product Popularity Plots ---\")\n",
    "\n",
    "models_to_evaluate_xgb = {\"XGBoost (Tuned)\": models_to_evaluate[\"XGBoost (Tuned)\"]}\n",
    "\n",
    "for name, model in models_to_evaluate_xgb.items():\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "\n",
    "    y_train_pred_log = model.predict(X_train_selected)\n",
    "    y_train_pred = np.expm1(y_train_pred_log)\n",
    "    y_train_true = np.expm1(y_train)\n",
    "\n",
    "    train_residuals = y_train_true - y_train_pred\n",
    "\n",
    "    y_test_pred_log = model.predict(X_test_selected)\n",
    "    y_test_pred = np.expm1(y_test_pred_log)\n",
    "    y_test_true = np.expm1(y_test)\n",
    "\n",
    "    test_residuals = y_test_true - y_test_pred\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    if 'Product_Popularity' in X_train_selected.columns:\n",
    "        axes[0].scatter(X_train_selected['Product_Popularity'], train_residuals, alpha=0.5)\n",
    "        axes[0].axhline(0, color='k', linestyle='--', lw=2)\n",
    "        axes[0].set_title(f'{name}: Residuals vs. Product Popularity (Train Set)')\n",
    "        axes[0].set_xlabel('Product Popularity')\n",
    "        axes[0].set_ylabel('Residuals (Actual - Predicted)')\n",
    "        axes[0].grid(True, linestyle='--', alpha=0.6)\n",
    "    else:\n",
    "        axes[0].set_title(f'{name}: Product_Popularity not in selected features (Train Set)')\n",
    "\n",
    "\n",
    "    if 'Product_Popularity' in X_test_selected.columns:\n",
    "        axes[1].scatter(X_test_selected['Product_Popularity'], test_residuals, alpha=0.5, color='salmon')\n",
    "        axes[1].axhline(0, color='k', linestyle='--', lw=2)\n",
    "        axes[1].set_title(f'{name}: Residuals vs. Product Popularity (Test Set)')\n",
    "        axes[1].set_xlabel('Product Popularity')\n",
    "        axes[1].set_ylabel('Residuals (Actual - Predicted)')\n",
    "        axes[1].grid(True, linestyle='--', alpha=0.6)\n",
    "    else:\n",
    "         axes[1].set_title(f'{name}: Product_Popularity not in selected features (Test Set)')\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2KpVQSzFh-0"
   },
   "source": [
    "Inference:\n",
    "\n",
    "XGBoost (Tuned):\n",
    "\n",
    "* Similar to the RandomForest model, the residuals vs. Product Popularity plots for XGBoost also show a tendency for the spread of residuals to increase with higher product popularity. This indicates that XGBoost, too, has more difficulty accurately predicting sales for very popular products.\n",
    "* The pattern of heteroscedasticity (increasing variance of residuals) is visible in both the training and test sets.\n",
    "* While 'Product_Popularity' is a significant feature, the model doesn't perfectly capture its relationship with sales, leading to larger errors for high-popularity items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T1UbXXUDFhfC"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "name = \"XGBoost (Tuned)\"\n",
    "original_pipeline = models_to_evaluate[name]\n",
    "\n",
    "print(f\"\\n--- Learning Curve Plot for {name} ---\")\n",
    "\n",
    "if isinstance(original_pipeline.named_steps['model'], TransformedTargetRegressor):\n",
    "    base_xgb_regressor = original_pipeline.named_steps['model'].regressor\n",
    "    xgb_params = base_xgb_regressor.get_params()\n",
    "    if 'early_stopping_rounds' in xgb_params:\n",
    "        del xgb_params['early_stopping_rounds']\n",
    "    xgb_regressor_for_lc = XGBRegressor(**xgb_params)\n",
    "    model_for_lc = TransformedTargetRegressor(\n",
    "        regressor=xgb_regressor_for_lc,\n",
    "        transformer=original_pipeline.named_steps['model'].transformer\n",
    "    )\n",
    "else:\n",
    "    base_xgb_regressor = original_pipeline.named_steps['model']\n",
    "    xgb_params = base_xgb_regressor.get_params()\n",
    "    if 'early_stopping_rounds' in xgb_params:\n",
    "        del xgb_params['early_stopping_rounds']\n",
    "    model_for_lc = XGBRegressor(**xgb_params)\n",
    "\n",
    "\n",
    "estimator = Pipeline(steps=[\n",
    "    ('preprocessor', original_pipeline.named_steps['preprocessor']),\n",
    "    ('model', model_for_lc)\n",
    "])\n",
    "\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    estimator,\n",
    "    X_train_selected, y_train,\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10)\n",
    ")\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "\n",
    "plt.title(f'{name}: Learning Curve')\n",
    "plt.xlabel(\"Training Examples\")\n",
    "plt.ylabel(\"R2 Score\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAjGYCleFspv"
   },
   "source": [
    "Inference:\n",
    "\n",
    "XGBoost (Tuned):\n",
    "\n",
    "* The learning curve shows how the model's performance (R2 score) changes as the size of the training data increases.\n",
    "* The training score starts high with a small number of training examples and decreases as more data is added. This is typical, as the model finds it harder to perfectly fit a larger, more diverse dataset.\n",
    "* The cross-validation score (performance on unseen data during cross-validation) starts low and increases as the training set size grows. This indicates that providing more data generally improves the model's ability to generalize.\n",
    "* There is a significant gap between the training score and the cross-validation score, even with the full training dataset. This suggests that the model might be overfitting the training data. However, the cross-validation score appears to be leveling off, indicating that adding significantly more data might yield diminishing returns for performance improvement with the current model configuration. Further tuning or regularization might be needed to reduce the overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eEFFO_vfFhYu"
   },
   "outputs": [],
   "source": [
    "name = \"XGBoost (Tuned)\"\n",
    "model = models_to_evaluate[name]\n",
    "\n",
    "print(f\"\\n--- Predicted vs. Actual by Category Plots for {name} ---\")\n",
    "\n",
    "y_train_pred_log = model.predict(X_train_selected)\n",
    "y_train_pred = np.expm1(y_train_pred_log)\n",
    "y_train_true = np.expm1(y_train)\n",
    "\n",
    "y_test_pred_log = model.predict(X_test_selected)\n",
    "y_test_pred = np.expm1(y_test_pred_log)\n",
    "y_test_true = np.expm1(y_test)\n",
    "\n",
    "X_train_selected_plot = X_train_selected.copy()\n",
    "X_train_selected_plot['Actual Sales'] = y_train_true\n",
    "X_train_selected_plot['Predicted Sales'] = y_train_pred\n",
    "\n",
    "X_test_selected_plot = X_test_selected.copy()\n",
    "X_test_selected_plot['Actual Sales'] = y_test_true\n",
    "X_test_selected_plot['Predicted Sales'] = y_test_pred\n",
    "\n",
    "\n",
    "if 'Category' in X_train_selected_plot.columns:\n",
    "    categories = X_train_selected_plot['Category'].unique()\n",
    "    n_categories = len(categories)\n",
    "\n",
    "    n_cols = 2\n",
    "    n_rows = (n_categories + n_cols - 1) // n_cols\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, n_rows * 5))\n",
    "    axes = axes.flatten()\n",
    "    for i, category in enumerate(categories):\n",
    "        ax = axes[i]\n",
    "\n",
    "        train_cat_data = X_train_selected_plot[X_train_selected_plot['Category'] == category]\n",
    "        test_cat_data = X_test_selected_plot[X_test_selected_plot['Category'] == category]\n",
    "\n",
    "        ax.scatter(train_cat_data['Actual Sales'], train_cat_data['Predicted Sales'], alpha=0.5, label='Train')\n",
    "        ax.scatter(test_cat_data['Actual Sales'], test_cat_data['Predicted Sales'], alpha=0.5, label='Test', color='salmon')\n",
    "\n",
    "\n",
    "        max_sales = max(train_cat_data['Actual Sales'].max(), test_cat_data['Actual Sales'].max())\n",
    "        ax.plot([0, max_sales], [0, max_sales], 'k--', lw=2)\n",
    "\n",
    "        ax.set_title(f'{category}')\n",
    "        ax.set_xlabel('Actual Sales')\n",
    "        ax.set_ylabel('Predicted Sales')\n",
    "        ax.legend()\n",
    "        ax.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.suptitle(f'{name}: Predicted vs. Actual Sales by Category', y=1.02, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\" 'Category' feature not found in selected features. Skipping this plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKtcBphyF3-p"
   },
   "source": [
    "Inference:\n",
    "\n",
    "XGBoost (Tuned):\n",
    "\n",
    "* The plots show the performance of the model on predicting sales within each product category.\n",
    "* For categories with a wider range of sales values (e.g., Technology, Furniture), the spread of predicted values is larger, but the model still tends to under-predict the highest sales values, consistent with the overall predicted vs. actual plot.\n",
    "* For categories with generally lower sales values (e.g., Office Supplies), the predictions are more clustered, and the model appears to perform reasonably well, although some under-prediction of slightly higher values within these categories is also visible.\n",
    "* The test set points generally follow a similar pattern to the training set within each category, but the spread around the diagonal line is wider, indicating the model is less accurate on unseen data for all categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kqGV5XaBFhTX"
   },
   "outputs": [],
   "source": [
    "name = \"XGBoost (Tuned)\"\n",
    "model = models_to_evaluate[name]\n",
    "\n",
    "print(f\"\\n--- Residuals vs Predicted Values Plots for {name} ---\")\n",
    "\n",
    "y_train_pred_log = model.predict(X_train_selected)\n",
    "y_train_pred = np.expm1(y_train_pred_log)\n",
    "y_train_true = np.expm1(y_train)\n",
    "\n",
    "train_residuals = y_train_true - y_train_pred\n",
    "\n",
    "y_test_pred_log = model.predict(X_test_selected)\n",
    "y_test_pred = np.expm1(y_test_pred_log)\n",
    "y_test_true = np.expm1(y_test)\n",
    "\n",
    "test_residuals = y_test_true - y_test_pred\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "axes[0].scatter(y_train_pred, train_residuals, alpha=0.5)\n",
    "axes[0].axhline(0, color='k', linestyle='--', lw=2)\n",
    "axes[0].set_title(f'{name}: Residuals vs. Predicted Values (Train Set)')\n",
    "axes[0].set_xlabel('Predicted Sales')\n",
    "axes[0].set_ylabel('Residuals (Actual - Predicted)')\n",
    "axes[0].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "axes[1].scatter(y_test_pred, test_residuals, alpha=0.5, color='salmon')\n",
    "axes[1].axhline(0, color='k', linestyle='--', lw=2)\n",
    "axes[1].set_title(f'{name}: Residuals vs. Predicted Values (Test Set)')\n",
    "axes[1].set_xlabel('Predicted Sales')\n",
    "axes[1].set_ylabel('Residuals (Actual - Predicted)')\n",
    "axes[1].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12p8SXSNGBpQ"
   },
   "source": [
    "Inference:\n",
    "\n",
    "XGBoost (Tuned):\n",
    "\n",
    "* Training Set: The residuals plot shows a cone shape, widening as predicted sales increase. This indicates heteroscedasticity, meaning the model's errors are larger for higher predicted sales values. The points are somewhat centered around zero, suggesting the model is not consistently over or under-predicting across the entire range of predicted values, although the spread of errors is not uniform.\n",
    "* Test Set: The pattern is similar to the training set, with residuals spreading out for higher predicted values. This confirms the heteroscedasticity issue on unseen data. The overall magnitude of residuals seems larger on the test set compared to the training set, which is consistent with the lower R2 and higher RMSE on the test set, indicating the model performs worse on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_DmfpTIzFhN7"
   },
   "outputs": [],
   "source": [
    "name = \"XGBoost (Tuned)\"\n",
    "model = models_to_evaluate[name]\n",
    "\n",
    "print(f\"\\n--- Distribution of Predicted Sales Values Plots for {name} ---\")\n",
    "\n",
    "y_train_pred_log = model.predict(X_train_selected)\n",
    "y_train_pred = np.expm1(y_train_pred_log)\n",
    "\n",
    "y_test_pred_log = model.predict(X_test_selected)\n",
    "y_test_pred = np.expm1(y_test_pred_log)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "axes[0].hist(y_train_pred, bins=50, edgecolor='k', alpha=0.7)\n",
    "axes[0].set_title(f'{name}: Distribution of Predicted Sales Values (Train Set)')\n",
    "axes[0].set_xlabel('Predicted Sales Values')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "\n",
    "axes[1].hist(y_test_pred, bins=50, edgecolor='k', alpha=0.7, color='salmon')\n",
    "axes[1].set_title(f'{name}: Distribution of Predicted Sales Values (Test Set)')\n",
    "axes[1].set_xlabel('Predicted Sales Values')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xx0nBHhZGO6r"
   },
   "source": [
    "Inference:\n",
    "\n",
    "XGBoost (Tuned):\n",
    "\n",
    "* Training Set: The distribution of predicted sales values for the training set appears to be right-skewed, with a concentration of predictions at lower sales values and a long tail extending towards higher values. This mirrors the distribution of the actual sales data, which is expected.\n",
    "* Test Set: The distribution of predicted sales values for the test set shows a similar right-skewed pattern. The range of predicted values on the test set might be slightly narrower or have fewer extreme high values compared to the training set, which could contribute to the performance drop on unseen data. The overall shape is consistent, which is a good sign that the model is learning the general distribution, even if the accuracy on individual high values is lower (as seen in the residuals plot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CF-GTGLLFhIT"
   },
   "outputs": [],
   "source": [
    "name = \"XGBoost (Tuned)\"\n",
    "\n",
    "print(f\"\\n--- Final Model Plots for {name} ---\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "final_results_df.loc[[name]][['Train R2', 'Test R2']].plot(kind='bar', ax=axes[0])\n",
    "axes[0].set_title(f'{name}: R2 Scores (Train vs Test)')\n",
    "axes[0].set_ylabel('R2 Score')\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "axes[0].legend(title='Dataset')\n",
    "\n",
    "final_results_df.loc[[name]][['Train RMSE', 'Test RMSE']].plot(kind='bar', ax=axes[1], color=['lightcoral', 'salmon'])\n",
    "axes[1].set_title(f'{name}: RMSE Scores (Train vs Test)')\n",
    "axes[1].set_ylabel('RMSE')\n",
    "axes[1].tick_params(axis='x', rotation=0)\n",
    "axes[1].legend(title='Dataset')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ySweJS4NFhDM"
   },
   "outputs": [],
   "source": [
    "name = \"XGBoost (Tuned)\"\n",
    "model = models_to_evaluate[name]\n",
    "\n",
    "print(f\"\\n--- Predicted vs. Actual Sales Plots for {name} ---\")\n",
    "\n",
    "y_train_pred_log = model.predict(X_train_selected)\n",
    "y_train_pred = np.expm1(y_train_pred_log)\n",
    "y_train_true = np.expm1(y_train)\n",
    "\n",
    "y_test_pred_log = model.predict(X_test_selected)\n",
    "y_test_pred = np.expm1(y_test_pred_log)\n",
    "y_test_true = np.expm1(y_test)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "axes[0].scatter(y_train_true, y_train_pred, alpha=0.5)\n",
    "axes[0].plot([y_train_true.min(), y_train_true.max()], [y_train_true.min(), y_train_true.max()], 'k--', lw=2)\n",
    "axes[0].set_title(f'{name}: Predicted vs. Actual Sales (Train Set)')\n",
    "axes[0].set_xlabel('Actual Sales')\n",
    "axes[0].set_ylabel('Predicted Sales')\n",
    "axes[0].set_xlim([0, y_train_true.max()])\n",
    "axes[0].set_ylim([0, y_train_true.max()])\n",
    "\n",
    "\n",
    "axes[1].scatter(y_test_true, y_test_pred, alpha=0.5, color='salmon')\n",
    "axes[1].plot([y_test_true.min(), y_test_true.max()], [y_test_true.min(), y_test_true.max()], 'k--', lw=2)\n",
    "axes[1].set_title(f'{name}: Predicted vs. Actual Sales (Test Set)')\n",
    "axes[1].set_xlabel('Actual Sales')\n",
    "axes[1].set_ylabel('Predicted Sales')\n",
    "axes[1].set_xlim([0, y_test_true.max()])\n",
    "axes[1].set_ylim([0, y_test_true.max()])\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "huLabnEBFg9Q"
   },
   "outputs": [],
   "source": [
    "name = \"XGBoost (Tuned)\"\n",
    "model = models_to_evaluate[name]\n",
    "\n",
    "print(f\"\\n--- Residuals Plots for {name} ---\")\n",
    "\n",
    "y_train_pred_log = model.predict(X_train_selected)\n",
    "y_train_pred = np.expm1(y_train_pred_log)\n",
    "y_train_true = np.expm1(y_train)\n",
    "\n",
    "train_residuals = y_train_true - y_train_pred\n",
    "\n",
    "y_test_pred_log = model.predict(X_test_selected)\n",
    "y_test_pred = np.expm1(y_test_pred_log)\n",
    "y_test_true = np.expm1(y_test)\n",
    "\n",
    "test_residuals = y_test_true - y_test_pred\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "axes[0].hist(train_residuals, bins=50, edgecolor='k', alpha=0.7)\n",
    "axes[0].set_title(f'{name}: Distribution of Training Residuals')\n",
    "axes[0].set_xlabel('Training Residuals')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "\n",
    "axes[1].hist(test_residuals, bins=50, edgecolor='k', alpha=0.7, color='salmon')\n",
    "axes[1].set_title(f'{name}: Distribution of Test Residuals')\n",
    "axes[1].set_xlabel('Test Residuals')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k9fG3TVdFg19"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0f51ad86f2a849eab83ad481d7c024c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "181ccdd28fc846d281586c7d3aa42a31": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4c33154b617b43109c76eb8ba8ca743d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4ef3f9db8dc14c3386297539e5a30e65": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "521996eba56c4b16800f6e2b53af703f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f0f28f0c6dad4da0ad05872985b51d30",
      "placeholder": "",
      "style": "IPY_MODEL_0f51ad86f2a849eab83ad481d7c024c5",
      "value": "200/200[06:11&lt;00:00,1.74s/it]"
     }
    },
    "52497735278941288115fc9f42d6a5bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_83faca0413e44c9a8a582605785f73a1",
      "max": 200,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4ef3f9db8dc14c3386297539e5a30e65",
      "value": 200
     }
    },
    "574524bd5af546ac8db523bd3982daa9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5bbf9f16d5184a319b9c24c4d4245a12": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_79781812c89d4b8fb05d1ed37575f259",
       "IPY_MODEL_e0aedbe67b09404b943f31a5b4726cd6",
       "IPY_MODEL_f06f3b24e5bf419ab62cd905712a30f4"
      ],
      "layout": "IPY_MODEL_6de03e2c037741fba39df3a97a464f6c"
     }
    },
    "6de03e2c037741fba39df3a97a464f6c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "71bdfbbea2114f238cd53726dd88c3af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "79781812c89d4b8fb05d1ed37575f259": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d75a2dd175a74c078ac58f80c24ce518",
      "placeholder": "",
      "style": "IPY_MODEL_181ccdd28fc846d281586c7d3aa42a31",
      "value": "100%"
     }
    },
    "83faca0413e44c9a8a582605785f73a1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "94047c5bc710410f9a439574a91086e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bbc2e2864d08416a83fbec6d43cd0c83": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f7ac3f718d7d4259a5cf1cfc11680528",
       "IPY_MODEL_52497735278941288115fc9f42d6a5bd",
       "IPY_MODEL_521996eba56c4b16800f6e2b53af703f"
      ],
      "layout": "IPY_MODEL_df7ed105bae04b92b791285046802819"
     }
    },
    "c04180644bd043eb91e5a236d5a2b2d2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d75a2dd175a74c078ac58f80c24ce518": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "df7ed105bae04b92b791285046802819": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e0aedbe67b09404b943f31a5b4726cd6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_574524bd5af546ac8db523bd3982daa9",
      "max": 200,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4c33154b617b43109c76eb8ba8ca743d",
      "value": 200
     }
    },
    "eb1ddbce18554c3787cd9fab791d5a8a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f06f3b24e5bf419ab62cd905712a30f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eb1ddbce18554c3787cd9fab791d5a8a",
      "placeholder": "",
      "style": "IPY_MODEL_71bdfbbea2114f238cd53726dd88c3af",
      "value": "200/200[10:05&lt;00:00,3.11s/it]"
     }
    },
    "f0f28f0c6dad4da0ad05872985b51d30": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f7ac3f718d7d4259a5cf1cfc11680528": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c04180644bd043eb91e5a236d5a2b2d2",
      "placeholder": "",
      "style": "IPY_MODEL_94047c5bc710410f9a439574a91086e4",
      "value": "100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
